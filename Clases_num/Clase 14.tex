\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}


%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{tikz,pgfplots}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{dsfont}
\usepackage{cancel}
\usepackage{tikz,tkz-base,tkz-fct}
\usepackage{pgfplots}
\newcommand{\prob}{\mathbb{P}}
\newtheorem{definicion}{Definición}
\newtheorem{teorema}{Teorema}
\newtheorem{ejemplo}{Ejemplo}
\DeclareMathOperator*{\argmax}{Arg\,max}
\newtheorem{lem}{Lema}
\newtheorem{prop}{Proposici\'on}
\newtheorem{cor}{Corolario}
\newtheorem{dem}{Demostración}
\numberwithin{equation}{subsection}
\numberwithin{definicion}{subsection}
\newtheorem{obs}{Observación}


%% Aquí se pueden definir nuevas abreviaturas para algunos comandos

\def\sen{{\rm sen\mspace{1.5mu}}}
\def\C{\mathbb C}
\def\R{\mathbb R}
\def\N{\mathbb N}
\def\Q{\mathbb Q}
\def\Z{\mathbb Z}
\def\V{\mathbb V}
\def\E{\mathbb E}
\def\to{\rightarrow}
\newcommand{\pb}{\mathbb{P}}



\newcommand{\ds}{\displaystyle}


%Para hacer normas en tex

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\normm}[1]{\bigg\lVert#1\bigg\rVert}


%integrales bacanes
\usepackage{ esint }

%Para poner en negrita en modo matemático
\newcommand{\negri}{\boldsymbol}




\title{Simulación Estocástica}
\author{Clase 14}
\date{05 de septiembre de 2019}

\begin{document}
\maketitle

El siguiente resultado entrega una taza de convergencia hacia la probabilidad invariante.

\begin{teorema}
Supongamos que $P$ es irreducible y satisface la condición de Doeblin $(D)$. Entonces $P$ es aperiódica, recurrente positiva, y si $\pi$ denota una probabilidad invariante, 
\[\sum_{y\in E}|(P^n)_{xy}-\pi_y| \leq 2(1-\beta)^{[n/n_0]},\hspace{0.3cm}\forall\,x\in E,\,\,n\in \N,\]
donde $[n/n_0]$ es la parte entera de $n/n_0$.
\end{teorema}

\textbf{Demostración: }Como la cadena es irreducible, la condición de Doeblin $(D)$ claramente implica que es aperiódica.
\begin{itemize}
    \item[Paso 1.] Primero probar que para cualesquiera dos probabilidades $\mu$ y $\nu$ en $E$,
    \begin{equation}
        ||\mu P^n - \nu P^n||_1 \leq 2(1-\beta)^{[n/n_0].}
        \label{clase14_dem1}
    \end{equation}
    Para probar esto, gracias al lema anterior, es suficiente contruir un coupling $(X_n,Y_n)$ de las probabilidades $\mu P^n$ y $\nu P^n$ tal que
    \[\prob(X_n \neq Y_n) \leq (1-\beta)^{[n/n_0]}.\]
    Supongamos que  $n = kn_0 + m$, con $m<n_0$. Dados $(X-_0,Y_0)$ con la ley $\mu \times \nu$ sobre $E\times E$, para $l = 0,1,2,\dcots,k-1$, definimos $(X_{(l+1)n_0},Y_{(l+1)n_0})$ en términos de $(X_{ln_0},Y_{ln_0})$ de la siguiente manera. Sea $\{\xi_l , U_l, V_l;\,l\geq 0\}$una sucesión de variables aleatorias mutuamente independientes, donde $\xi_l$ es una variable $Bernoulli$ con $\prob(\xi_l = 1)=\beta = 1 -\prob(\xi_l = 0)$, la ley de $U_l$ es $\Bar{m} = \beta^{-1}m$ y $V_l$ es uniforme en $[0,1]$. Definimos
    \[Q_{xy} = (-1\beta)^{-1}((P^{n_0})_{xy}-m_y)\]
    y $f:E\times [0,1] \rightarrow E$ tal que, para todo $x,y\in E$, $\{u;\,f(x,u)=y\}$ es un boreliano subconjunto de $[0,1]$, y dado que $V$ es uniforme en $[0,1]$, la ley de $f(x,V)$ es $Q_{x\cdot}$, $x\in E$. Ahora sean
    \[X_{(l+1)n_0} = \xi_l U_l + (1-\xi_l)f(X_{ln_0},V_l),\]
    \[Y_{(l+1)n_0} = \xi_l U_l + (1-\xi_l)f(Y_{ln_0},V_l).\]
    Notamos que, realmente, hemos contruído un coupling $(X_{ln_0},Y_{ln_0})$ de $\mu P^{ln_0}$ y $\nu P^{ln_0}$, para $l=0,cdtos,k$, tales que
    \[\prob(X_{ln_0} \neq Y_{ln_0}) \leq  \prob(\cap_{m=0}^l \xi_m =0) = (1-\beta)l.\]
    Resta por construir un coupling $(X_n,Y_n)$ de $\mu P^n$ y $\nu P^n$, tal que $\{X_n \neq Y_n\} \subset \{X_{kn_0} \neq Y_{kn_0}\}$, lo  que es fácil.
    \item[Paso 2.] Ahora probaremos que cualquier probabilidad $\mu$ sobre $E$, $\{\mu P^n;\,n\geq 0\}$ es una sucesión de Cauchy en el espacio de Banach, $l^1(E)$. Si $\nu = \mu P^m$, por \ref{clase14_dem1} 
    \[||\mu P^{n+m}-\mu P^n||_1 = ||\nu P^n - \mu P^n||_1 \leq 2c^{n-n_0},\]
    donde $c=(1-\beta)^{1/n_0}$. Se concluye.
    \item[Paso 3.] Del paso anterior tenemos que la secuencia de probabilidades $\{\mu P^n;\,n\geq 0\}$ converge en $l^1(E)$, hacia $\pi$, probabilidad en $E$. Pero
    \[\pi P = \lim_{n\rightarrow \infty}\mu P^{n+1} = \pi,\]
    por lo tanto $\pi$ es invariante, y la cadena es recurrente positiva. En consecuencia, de \ref{clase14_dem1}, para cada probabilidad $\mu$ de $E$,
    \[||\mu P^n - \pi||_1 \leq 2(1-\beta)^{[n/n_0]},\]
    lo que establece la razón de convergencia buscado, y la aperiodicidad.
\end{itemize}
\rule{0.7em}{0.7em}\\ \newline
Ahora enunciaremos un teorema central del límite para cadenas de Markov irreducibles, recurrentes positivas. Si tal cadena, además satisface
\[\sum_{y \in E}|(P^n)_{xy}-\pi_y| \leq Mt^n,\hspace{0.3cm}x \in E,\,n\in \N\]
con $M\in \R$ y $0<t<1$, es llamada \textit{uniformemente ergódica}. Sólamente debemos mostar que la condición de Doeblin implica uniforme ergodicidad. Esta propiedad implica el teorema centrar del límite.

\begin{teorema}
Sea $\{X_n;\,n\in \N\}$ es una cadena de Markov a valores de $E$, con matriz de de transición $P$ irreducible, que además es uniformemente ergódica y aperiódica. Sea $\pi$ la única distribución invariante de la cadena, y $f:E\rigtarrow \R$, tal que
\[\sum_{x\in E}\pi_x f^2(x) < \infty \hspace{0.2cm}\text{y}\hspace{0.2cm}\sum_{x \in E}\pi_x f(x) = 0.\]
Entonces, cuando $n\rigtarrow \infty$,
\[\frac{1}{\sqrt{n}}\sum_1^nf(X_k)\hspace{0.2cm}\xrightarrow{\,\,\mathcal{L}\,\,}\sigma_f Z,\]
donde $Z\sim \mathcal{N}(0,1)$ y
\[\sigma_f^2 = \sum_{x \in E}\pi_x (Qf)_x^2 - \sum_x \pi_x (PQf)_x^2\]
\[= 2\sum_x \pi_x (Qf)_x f_x - \sum_x \pi_x f_x^2,\]
con 
\[(Qf)_x = \sum_{n=0}^{\infty}\E_x\left[f(X_n)\right],\hspace{0.2cm}x\in E.\]
\end{teorema}

Note que la propiedad uniforme ergodicidad implica que la serie que define al operador $Q$ converja.

\subsection{Cadenas de Markov Reversibles}
Consideremos el caso irreducible y recurrente positivo. La propiedad de Markov (aquella que dice que el pasado y el futuro de la cadena son independientes condicionado al valor presente) nos dice que, cuando $\{X_n;\,n\in \N\}$ es una cadena de Markov, para cada cada $N$, $\{\hat{X}_n^N = X_{N-n};\,0\leq n \leq N\}$ también es cadena de Markov, a menos que $\{X_n\}$ inicie con probabilidad invariante $\pi$.
\begin{prop}
Sea $\{X_n;\,n\in\N\}$ una cadena de Markov con matriz de transición $P$, que supondremos irreducible, y $\pi$ su distribución invariante. Entonces la cadena a tiempo-reverso\\ $\{\hat{X}_n^N;\,0\leq n\leq N\}$ es una $(\pi,\hat{P})$-C.M., con
\[\pi_y \hat{P}_{yx} = \pi_x P_{xy},\hspace{0.3cm}x,y\in E.\]
\end{prop}

\textbf{Demostración: }
\[\prob(\hat{X}_{p+1} = x\,|\,\hat{X}_p = y)\]
\[= \prob(X_n =x\,|\,X_{n+1}=y)\]
\[= \prob(X_{n+1} = y\,|\,X_n=x)\times \frac{\prob(X_n=x)}{\prob(X_{n+1}=y)}\]
\rule{0.7em}{0.7em}\\ \newline

Decimos que la cadena $\{X_n;\,n\in\N\}$ es \textit{reversible} si $P=\hat{P}$, lo cual se satisface si y sólo si se satisfacen las \textit{ecuaciones de balance detallado}:
\[\pi_x P_{xy} = \pi_y P_{yx},\hspace{0.3cm}x,y\in E,\]
donde $\pi$ es la probabilidad invariante. Es fácil verificar que si $\pi$ satisface esa relación, entonces es $P$-invariante. La proposición contraria no necesariamente es cierta.\\ \newline
 
\textbf{Observación: }Si $\pi$ es la distribución invariante de una cadena de Markov irreducible (y, por lo tanto, recurrente positiva), la cadena no necesariamente es reversible. Suponga que $card(E)\geq 3$. Entonces podría existir $x\neq y$ tal que $P_{xy}=0\neq P_{yx}$. En consecuencia, $\pi_xP_{xy}=0\neq \pi_yP_{yx}$. La transición de $y$ a $x$ de la cadena original corresponde a la transición de $x$ a $y$ de la cadena a tiempo inverso, por lo tanto $P_{yx}\neq 0 \Longrightarrow \hat{P}_{xy} \neq 0$, de donde se concluye que $\hat{P}\neq P$.\\

\textbf{Observación: }Dada una matriz dde transición $P$ de una cadena de Markov irreducible, recurrente positiva, entonces una de las cosas que se busca es el cálculo de la probabilidad invariante. Este problema no es simpre resoluble.\\ 
Otro problema, es determinar una matriz de transición $P$, cuya cadena de Markov asociada admita una probabilidad $\pi$, invariante.\\ 
El segundo problema es bastante más fácil de resolver. En efecto, siempre existen varias soluciones. La manera más fácil de resolverlo es buscar $P$ tal que la cadena asociada es reversible con respecto a $\pi$. En otras palabras, es suficiente encontrar una matriz de transición $P$, irreducible, talq ue la cantidad $\pi_x P_{xy}$ sea simétrica en $x,y$.\\
A fin de resolver el primer problema, uno puede tratar de encontrar $\pi$ tal que
\[\pi_x P_{xy} = \pi_y P_{yx},\hspace{0.3cm}\forall\,x,y\in E,\]
lo que es diferente a resolver $\pi P= \pi$, no implica ninguna suma con respecto a $x$. Pero esta ecuación tiene solución sólo si la cadena es reversible con respecto a su única medida de probabilidad invariante, lo que podría no ser el caso.\\
Supongamos ahora que, dado el par $(P,\pi)$, deseamos verificar si $\pi$ es, o no, una probabilidad invariante con respecto a la matriz de transición $P$. Si la cantidad $\pi_x P_{xy}$ es simétrica en $x,y$, entonces la respuesta es que sí lo es, y ganamos una propiedad adicional, la reversibilidad. Si no es el caso, uno necesita verificar (o no) la igualdad $\pi P = \pi$. La siguiente proposición puede ser de utilidad para el problema de verificación.

\begin{prop}
Sea $P$ una matriz de transición irreducible, y $\pi$ una probabilidad en $E$, estrictamente positiva. Para cada par $x$, $y\in E$, definimos
\[\hat{P}_{xy}= \begin{cases}
                \frac{\pi_y}{\pi_x}P_{yx}, & \text{si }x\neq y,\\
                P_{xx}, & \text{si }x=y.
                \end{cases}\]
$\pi$ es probabilidad invariante de la cadena definida por la matriz de transición $P$, y $\hat{P}$ es la matriz de transición de la cadena a tiempo reverso si y sólo si, para cada $x\in E$,
\[\sum_{y\in E} \hat{P}_{xy} = 1.\]
\end{prop}

\subsection{Razón de convergencia al equilibrio}
Suponga el caso irreducible, recurrente positivo y aperiódico. Entonces, sabemos que para cada $x,y \in E$, $(Pn)_{x,y}\rightarrow pi_y$ cuando $n\rightarrow \infty$, donde $\pi$ es la única medida de probabilidad invariante. Más generalmente, esperamos que para una gran clase de funcione $f:E\rightarrow \R$, $(P^nf)_{x}\rightarrow \langle f,\pi \rangle$ cuando $n\rightarrow \infty$ para todo $x\in E$, donde, de ahora en adelante,
\[\langle f,\pi \rangle = \sum_{x\in E}f(x)\pi_x.\]
En esta sección discutiremos la razón a la cual esta convergencia se mantiene.

\subsubsection{El caso reversible de finitos estados}
En primer lugar, consideremos el caso simple. Asumamos que $E$ es finito (y escribimos $d=|E|$) y el proceso es reversible. Notemos que podemos identificar $L^2(\pi)$ con $\R^d$, dotado  del producto escalar
\[\langle f,g \rangle_{\pi} = \sum_{x\in E}f(x)g(x)\pi_x.\]
Ahora, la reversibilidad de $P$ es equivalente a decir que $P$ es, como elemento de $\mathcal{L}(L^2(\pi))$, un operador auto-adjunto, en el sentido que
\[\langle Pf,g\rangle_{\pi} = \sum_{x,y\in E}P_{x,y}f(y)g(x)\pi_x\]
\[= sum_{x,y\in E}P_{y,x}f(y)g(x)\pi_y\]
\[= \langle f,Pg \rangle_{\pi},\]
donde usamos la ecuación de balance detallado para la segunda identidad. Ahora veremos que la norma del operador $P$, como elemento de $\mathcal{L}(L^2(\pi))$, es almenos 1. En efecto, si $||\cdot ||_{\pi}$ denota la norma usual en $L^2(\pi)$,
\[||Pf||_{\pi}^2 = \sum_{x\in E}\left[(Pf)_x\right]^2\pi_x\]
\[ = \sum_{x\in E}(\E[f(X_t)\,|\,X_0=x])^2\pi_x\]
\[\leq \E[f^2(X_t),|\,X_0=x]\pi_x\]
\[=\sum_{x\im E}f^2(x)\pi_x,\]
donde se usó la desigualdad de Schwarz (o, equivalentemente, la desigualdad de Jensen) para la parte en que aparece la desigualdad, y la invarianza de $\pi$ para la última identidad.\\
A fin de poder trabajar en $\R^d$, dotado de la norma euclideana, introduciremos una nueva matriz $d\times d$
\[\Tilde{P}_{x,y} := \sqrt{\frac{\pi_x}{\pi_y}}P_{x,y}.\]
En notación matricial, $\Tilde{P}=\Pi^{1/2}P\Pi^{-1/2}$, donde $\Pi_{x,y} = \delta_{x,y}\pi_x$ es una matriz diagonal. Más aún, si $||\cdot ||$ es la norma euclideana en $\R^d$, para cada $f:\E\rightarrow \R$ (es decir, $f$ es una colección de números reales indexada por los $d$ elementos de $E$, en otras palabras un elemento de $\R^d$), anotando como $g = \Pi^{-1/2}f$, tenemos
\[||\Tilde{P}f||^2 = \sum_{x\in E}(P\Pi^{-1/2}f)_x^2 = ||Pg||_{\pi}^2 \leq ||g||_{\pi}^2 = ||f||^2.\]
Primero, notemos que $f$ es un vector propio de $\Tilde{P}$ si y sólo si $g=\Pi^{-1/2}f$ es un vector propio derecho de $P$, y $g' = \Pi^{1/2}f$ es un vector propio izquierdo de $P$ asociados al mismo valor propio. Tenemos que $\Tilde{P}$ es una $d\times d$ matriz simétrica, cuya norma está acotada por 1. Por lo tanto, por resultado elementales de álgebra lineal, $\Tilde{P}$ admite los valores propios $-1\leq \lambda_d \leq \lambda_{d-1}\leq \lambda_2\leq \lambda_1\leq 1$. Establecemos el siguiente lema.
\begin{lem}
Tenemos que $\lambda_2 <\lambda_1 = 1$ y $-1<\lambda_d$.
\end{lem}

\textbf{Demostración: }Ver \cite[Pag. 41]{Pard}\\ \newline

Sean $g_1, \cdots, g_d$ la base ortonormal de $L^2(\pi)$ hecha por vectores propios derechos de $P$, correspondientes respectivamente a los valores propos $1,\lambda_1,\cdots, \lambda_d$. Para cada $f\in L^2(\pi)$, dado $g_1 = (1,\cdots,1)$,
\[f-\langle f,\pi \rangle = \sum_{l=2}^d \langle f,g_l \rangle_{\pi}g_l,\]
\[Pf - \langle f,\pi \rangle = \sum_{l=2}^d \lambda_l \langle f,g_l \rangle_{\pi}g_l,\]
\[P^nf - \langle f,\pi \rangle = \sum_{l=2}^d \lambda_l^n \langle f,g_l \rangle_{\pi}g_l,\]
\[|| P^nf -\langle f,\pi \rangle||_{\pi}^2 = \sum_{l=2}^d \lambda_l^{2n}\langle f,g_l\rangle_{\pi}^2,\]
\[\leq \sup_{2\leq l\leq d}\lambda_{l}^{2n}||f-\langle f,\pi \rangle ||_{\pi}^2,\]
por lo tanto tenemos la siguiente proposición.
\begin{prop}
\[||P^nf - \langle f,\pi \rangle ||_{\pi} \leq (1-\beta)^n ||f-\langle f,\pi \rangle ||_{\pi},\]
donde $\beta := (1-\lambda_2)\wedge (1+\lambda_d)$ es la brecha espectral.
\end{prop}

\subsubsection{El caso general}
Más generalmente, lo mismo es cierto para
\[\beta := 1 - \sup_{f\in L^2(pi),\,||f||_{\pi} = 1} ||Pf - \lange f,pi \rangle||_{\pi}.\]
En eecto, con este $\beta$, considerando sólo el caso $f\neq 0$, dado que todas las desigualdades a conticnuación son ciertas para $f=0$, tenemos
\[||Pf -\langle f,\pi \rangle ||_{\pi} = \left|\left| P \left(\frac{f}{||f||_\pi}\right) - \left\langle \frac{f}{||f||_{\pi}},\pi\right\rangle \right|\right|_{\pi} \times ||f||_{\pi}\]
\[\leq (1-\beta)||f||_{\pi}.\]
Finalmente, la proposición anterior se mantiene en el caso general para esta nueva definición de $\beta$. Note que
\[||P^{n+1}f - \langle f,\pi \rangle||_{\pi} = ||P[P^nf - \langle f,\pi \rangle]||_{\pi}\]
\[\leq (1-\beta)||P^nf- \langle f,\pi \rangle||_{\pi}.\]
El resultado se concluye mediante inducción.

\subsection{Estadísticas en cadenas de Markov}
El objetivo de esta sección es introducir las nociones básicas de la estimación de los parámetros de una cadena de Markov.\\
Hemos visto que, para cada $n\geq 0 $, la ley del vector aleatorio $(X_0,X_1,\cdots,X_n)$ depende sólo de la ley inicial $\mu$ y de la matriz de transición $P$. Estamos interesados en las condiciones bajo las cuales uno puede estimar el par $(\mu,P)$, dado un vector de observaciones de $(X_0,X_1,\cdots,X_n)$, y de orma en que el error tienda a cero cuando $n\rightarrow \infty$.\\
Primero, analicemos el estimador de la probabilidad invariante $\mu$. Para cada $x\in E$, 
\[\hat{\mu}_x^n = \frac{1}{n+1}\sum_{l=0}^n\mathbbm{1}_{\{X_l =x\}}\]
es un estimador consistente de $\mu_x$, esto es consecuencia directa del teorema ergódico:
\begin{prop}
Para cada $x\in E$, $\hat{\mu}_x^n \rightarrow \mu_x$, casi seguramente cuando $n\rightarrow \infty$.
\end{prop}

Ahora observemos el estimador de $P_{xy}$ $,x,y\in E$. Elegimos el estimador
\[\hat{P}_{xy}^n = \frac{\sum_{l=0}^{n-1}\mathbbm{1}_{\{X_l= x,X_{l+1}=y\}}}{\sum_{l=0}^{n-1}\mathbbm{1}_{\{X_l =x\}}}.\]
Tenemos la siguiente proposición.
\begin{prop}
Para cada $x,y\in E$, $\hat{P}_{xy}^n \rightarrow P_{xy}$, casi seguramente cuando $n\rigtarrow \infty$.
\end{prop}

\textbf{Demostración: }Ver \cite[Pag. 43]{Pard}\\ \newline
\end{document}