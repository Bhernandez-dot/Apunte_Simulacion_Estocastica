\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}


%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{tikz,pgfplots}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{dsfont}
\usepackage{cancel}
\usepackage{tikz,tkz-base,tkz-fct}
\usepackage{pgfplots}
\newcommand{\prob}{\mathbb{P}}
\newtheorem{definicion}{Definición}
\newtheorem{teorema}{Teorema}
\newtheorem{ejemplo}{Ejemplo}
\DeclareMathOperator*{\argmax}{Arg\,max}
\newtheorem{lem}{Lema}
\newtheorem{prop}{Proposici\'on}
\newtheorem{cor}{Corolario}
\newtheorem{dem}{Demostración}
\numberwithin{equation}{subsection}
\numberwithin{definicion}{subsection}
\newtheorem{obs}{Observación}


%% Aquí se pueden definir nuevas abreviaturas para algunos comandos

\def\sen{{\rm sen\mspace{1.5mu}}}
\def\C{\mathbb C}
\def\R{\mathbb R}
\def\N{\mathbb N}
\def\Q{\mathbb Q}
\def\Z{\mathbb Z}
\def\V{\mathbb V}
\def\E{\mathbb E}
\def\to{\rightarrow}
\newcommand{\pb}{\mathbb{P}}



\newcommand{\ds}{\displaystyle}


%Para hacer normas en tex

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\normm}[1]{\bigg\lVert#1\bigg\rVert}


%integrales bacanes
\usepackage{ esint }

%Para poner en negrita en modo matemático
\newcommand{\negri}{\boldsymbol}




\title{Simulación Estocástica}
\author{Clase 8}
\date{19 de agosto de 2019}

\begin{document}
\maketitle

\textbf{Recuerdo:} Decíamos que si; $X$ es una variable aleatoria con densidad $f$, que no podemos simular directamente. Sea $Y$ variable aleatoria con densidad $g$ (eficientemente simulable) tal que:
\begin{enumerate}
    \item Existe $k$: $f(x) \leq kg(x)$ $\forall\,x\in\R$.
    \item $g(x)>0$ $\Longleftrightarrow$ $f(x)>0$.
\end{enumerate}
Definiendo $\alpha(x) = \frac{f(x)}{kg(x)}$ para todo $x$ tal que $g(x)>0$. Explicamos el método de Aceptación-Rechazo mediante el siguiente algoritmo:\\ \newline
\textbf{Algoritmo Aceptación-Rechazo:} para simular la variable $X$.
\begin{enumerate}
    \item Simular $U\sim unif(0,1)$, simular $Y\sim g$. Ambas ($U$ e $Y$) independientes entre si.
    \item Si $U\leq \alpha(Y)$, $X=Y$ es un muestreo de $X$.
    \item Si $U> \alpha(Y)$, volver al paso 1 e iterar.
\end{enumerate}
\newline
\textbf{Observación: }De la demostración previa notamos que; $N\sim geom(1/k)$. Entonces el valor esperado de la cantidad de iteraciones necesarias para simular un valor de $X$ es $\E(N) = k$. Luego, para que el método termine en un tiempo razonable, es necesario ajustar el valor $k$ lo más cercano a 1 posible, es decir; elegir una densidad $g$ suficientemente parecida a $f$.\\ \newline
\textbf{Observación: }El método anterior es válido, aún tomando densidades con respecto a una medida $\mu$ cualquiera. Es decir, para $f$ y $g$ densidades de $X$ e $Y$, respectivamente, que cumplen las condiciones:
\[\prob\left(X\in A\right) = \int_{A}f(x)\mu(dx) \leq \int_{A}kg(x)\mu(x) = k\prob\left(Y\in A\right)\]
En particular, si $X$ es una variable aleatoria discreta, digamos en $\N$, entónces $f(x) = \prob(X=x)$ es la densidad de $X$ con respecto a la medida \textit{cuenta-puntos}. Luego, si $Y$ es otra variable aleatoria discreta que cumple las hipótesis con su densidad, es posible aplicar el método.

\subsection{Reducción de varianza}
Recordemos que el método de Monte Carlo (M.M.C.) aproxima un valor
\begin{equation}
    \alpha = \E(X) \approx \frac{1}{n}\sum_{i=1}^{n}X_i
    \label{redvar_1}
\end{equation}
donde $X_1,\cdots,X_n$ son copias $i.i.d.$ de $X$. Además sabíamos que el error es del orden de
$\frac{\sigma}{\sqrt{n}}$, con $\sigma^2 = Var(X)$.\\
Notamos que el valor por estimar en $\ref{redvar_1}$ sólo depende de la esperanza de la variable $X$ y no en la variable en sí o su distribución, por lo tanto tenemos la libertad de cambiar la variable a utilizar por una que tenga menor varianza, así estimaremos el mismo valor con un error menor.

\subsubsection{Muestreo preferencial}
Sea $X$ con densidad $f$. Sea el valor a estimar
\[\alpha = \E(g(X))\]
donde $g:\R\rightarrow\R$ es una función dada.\\ \newline
Ahora, supongamos que tenemos $Y$, otra variable aleatoria con densidad $\Tilde{f}>0$, entonces:
\[\alpha = \E(g(X)) = \int g(x)f(x) dx = \int \frac{g(x)f(x)}{\Tilde{f}(x)}\Tilde{f}(x) dx \]
Si definimos la función $h$ como:
\[h(x) = \frac{g(x)f(x)}{\Tilde{f}(x)}\]
Entonces $\alpha$ es la esperanza de la función $h$ bajo la densidad $\Tilde{f}$, es decir $\alpha = \E(h(Y))$. Además:
\[Var(h(Y)) = \E\left(h(Y)^2\right) - \alpha^2\]
\[= \int \frac{g(x)^2f(x)^2}{\Tilde{f}(x)^2}\Tilde{f}(x)dx - \alpha^2\]
\[= \int \frac{f(x)^2g(x)^2}{\Tilde{f}(x)}dx - \alpha^2\]
Si $g\geq 0$, entonces si escogemos \begin{equation}
    \Tilde{f}(x) = \frac{g(x)f(x)}{\alpha}
    \label{eq.rv2}
\end{equation}
entonces, obtenemos $Var(h(Y)) = 0$.\\ Evidentemente, \ref{eq.rv2} no funciona en la práctica, pues no conocemos el valor de $\alpha$. Sin embargo, nos sugiere que sería óptimo tomar una función $\Tilde{f}$ suficientemente parecido, en cuanto a forma, a $g(x)f(x)$ (diferenciado por una pondderación constante).\\ \newline

Esto motiva el siguiente procedimiento heurístico, escogeremos $\Tilde{f}$ cumpliendo:
\begin{itemize}
    \item $\Tilde{f}(x)$ "parecido" a $|g(x)f(x)|$, normalizado para que integre 1.
    \item Sea fácil de simular una variable $Y$, con densidad $\Tilde{f}$.
\end{itemize}
Luego basta aproximar $\alpha \approx \frac{1}{n}\sum_{i=1}^n h(Y_i)$, con $Y_1,\cdots,Y_n$ copias $i.i.d.$ de $Y$.\\ \newline

\textbf{Ejemplo: }Queremos aproximar:
\[I = \int_{0}^1 cos\left(\frac{\pi x}{2}\right)dx = \frac{2}{\pi}\]
Suponiendo que no podemos calcular esta integral, difiniendo:
\[I = \E(g(X)),\hspace{1cm}g(x) = cos\left(\frac{\pi x}{2}\right)\]
con $X$ una variable aleatoria uniforme en el intervalo $[0,1]$, podemos calcular $I$ mediante $M.M.C.$.\\ \newline
En este caso, podríamos reemplazar la función $coseno$ por un polinomio de grado 2. Dado que el interior de la integral es 0 en $x=1$ y 1 en $x=0$, es natural elegir $\Tilde{f}(x)$ de la forma $\lambda(1-x^2)$. Si normalizamos la función en este intervalo, tenemos que $\Tilde{f}(x) = 3(1-x^2)/2$. Al calcular las varianzas, podemos notar que el método reduce la varianza en un factor de 100. \\
\newpage

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[width=7cm, height=5cm,xmin=-0.1,xmax=1.1,ymin=-0.01,ymax=1.7, xmajorgrids=true, ymajorgrids=true, xtick={0,0.25,0.5,0.75,1},  xticklabels={0,,,,1}, ytick={0,0.25,0.5,0.75,1,1.25,1.5}, yticklabels={0,,,,1,,1.5},samples =500]
        \addplot[blue, ultra thick] (x,1.5-1.5*x*x);
        \addplot[red, ultra thick](x,cos(deg(1.5707*x)));
        \end{axis}
    \end{tikzpicture}
    \caption{En rojo la función $cos(\pi x/2)$, en azul la función $3(1-x^2)/2$ mayorándola.}
\end{figure}\\

\subsubsection{Variable de control}
Este método involucra escribir $\E(f(X))$ de la forma:
\[\E\left(f(X)\right) = \E\left(f(X)-h(X)\right) + \E\left(h(X)\right)\]
Donde $\E\left(h(X)\right)$ puede ser calculado explícitamente, y $Var\left(f(X)-h(X)\right)$ es significatívamente menor que $Var\left(f(X)\right)$. Así, empleamos el método de Monte Carlo para el cálculo de $\E\left(f(X)-h(X)\right)$ y el cálculo directo de $\E\left(h(X)\right)$.\\ \newline
\textbf{Ejemplo: }Sumpogamos que deseamos calcular la siguiente integral:
\[I = \int_{0}^1 e^x dx = e-1\]
Aplicando el $M.M.C.$, con $I=\E(e^X) = \E(g(X))$, con $X\sim unif(0,1)$. Notamos que la varianza de la función es $Var(g(X)) \approx 0,242$.\\
Dado que, cerca de $x=0$, $e^x \approx 1+x$, podemos escribir:
\[\int_{0}^1 e^x dx = \int_{0}^1 (e^x -1-x)dx + \frac{3}{2}\]
En este caso, elegimos $h(x)=1+x$. Es fácil comprobar que $Var\left(g(x)-h(x)\right) \approx 0.0437$. Es decir, hemos reducido la varianza de la estimación en un factor de $5.5$ aproximadamente.
\end{document}