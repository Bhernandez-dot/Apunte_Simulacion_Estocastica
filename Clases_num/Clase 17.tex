\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}


%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{tikz,pgfplots}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{dsfont}
\usepackage{cancel}
\usepackage{tikz,tkz-base,tkz-fct}
\usepackage{pgfplots}
\newcommand{\prob}{\mathbb{P}}
\newtheorem{definicion}{Definición}
\newtheorem{teorema}{Teorema}
\newtheorem{ejemplo}{Ejemplo}
\DeclareMathOperator*{\argmax}{Arg\,max}
\newtheorem{lem}{Lema}
\newtheorem{prop}{Proposici\'on}
\newtheorem{cor}{Corolario}
\newtheorem{dem}{Demostración}
\numberwithin{equation}{subsection}
\numberwithin{definicion}{subsection}
\newtheorem{obs}{Observación}


%% Aquí se pueden definir nuevas abreviaturas para algunos comandos

\def\sen{{\rm sen\mspace{1.5mu}}}
\def\C{\mathbb C}
\def\R{\mathbb R}
\def\N{\mathbb N}
\def\Q{\mathbb Q}
\def\Z{\mathbb Z}
\def\V{\mathbb V}
\def\E{\mathbb E}
\def\to{\rightarrow}
\newcommand{\pb}{\mathbb{P}}



\newcommand{\ds}{\displaystyle}


%Para hacer normas en tex

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\normm}[1]{\bigg\lVert#1\bigg\rVert}


%integrales bacanes
\usepackage{ esint }

%Para poner en negrita en modo matemático
\newcommand{\negri}{\boldsymbol}




\title{Simulación Estocástica}
\author{Clase 17}
\date{01 de octubre de 2019}

\begin{document}
\maketitle

\subsection{Simulated Annealing}
La busqueda de máximos globales de alguna función es uno de los problemas más importatntes de la matemática aplicada. En el caso de funciones diferenciables definidas sobre $\R^d$, uno puede empezar por un punto arbitrario y moverse en la dirección determinada por el gradientehasta que la función detenga su crecimiento. Desafortunadamente, tales métodos conducen a mínimos locales, los cuales podría no ser mínimos globales. En el caso en que la función esté definida sobre un conjunto finito $E$, uno podría calcular cada valor de la función $f(x)$ en cada punto $x\in E$, pero la eficiencia de este procedimiento depende bastante del tamaño que tenga el conjunto $E$, en casos que le el número de elementos sea demasiado grande esto se hace imposible de implementar.\\
Usaremos aleatoriedad para escapar de óptimos locales, buscando en zonas distintas del espacio $E$, ojalá convergiendo al óptimo global. A medida que el algoritmo progresa, se irán reduciendo las perturbaciones aleatorias.\\ \newline
Específicamente: sea $E$ finito, sea $U:E\rightarrow \R_{-}$, y supongamos
\[\max_{x\in E}U_x = 0.\]
Buscamos $x\in E$ tal que $U_x = 0$. Para $\beta >0$ ($1/\beta$ es la \textit{temperatura}), sea $\pi^{\beta}\in \mathcal{P}(E)$ dad por
\[\pi_x^{\beta} = \frac{e^{\beta U_x}}{Z_\beta},\hspace{0.2cm}\text{con\footnote{Típicamente, es imposible calcular $Z_\beta$.} }Z_{\beta} := \sum_{x\in E}e^{\beta U_x}\]
Notamos que $\pi_x^\beta$ favorece a los $x\in E$ cuyos valores de $U_x$ sean más altos. Más aún:
\[\pi^\beta \xrightarrow{\beta \rightarrow \infty} \lambda \]
donde $\lambda$ es una medida uniforme sobre el conjunto de los máximos de $U$.\\ \newline

Para cada $\beta >0$, consideremos una matriz markoviana irreducible y aperiódica, tal que tiene a $\pi^\beta$ como distribución invariante. \\ Sea $G$ un grafo no dirigido sobre $E$, conexo, y sea
\[n_x = |\{y \in E\,:\,xy \in G\}|\]
\[= \#\left(\{\text{vecinos de }x\}\right),\]
y definimos; para $x\neq y$
\[P_{xy}^\beta = \begin{cases}
1/n_x\left(e^{\beta(U_y-U_x)}\wedge 1\right), & xy\in G\\
0, & \sim
\end{cases}\]
y para $x=y$
\[P_{xx}^\beta = 1-\sum_{y \neq x}P_{xy}^\beta. \]
Vemos que $P^\beta$ hace improbables a las transiciones de estados que hacen disminuir el valor de $U$.\\ Es fácil ver que $\pi^\beta$ es invariante para $P^\beta$. Luego, si $(X_n^\beta)_{n\in \N}$ es una $P^\beta$-CMH, su ley converge a $\pi^\beta$ cuando $n\rightarrow \infty$. \\ \newline
\textbf{SIMULATED ANNEALING: }Tomar $\beta = \beta_n$, con $\beta_n \rightarrow \infty$, lentamente.\\ \newline
Usualmente
\[\beta_n = \frac{1}{c}ln(n+e),\]
con $c>0$, una constante a calibrar. Esto genera
\[(X_n)_{n\geq0} = (X_n^{\beta_n})_{n\geq 0},\]
Lo cual es una cadena de Markov \textit{no-homogénea}.\\La idea es tomar $c$ suficientemente grande, de manera que $X_n$ converga a un máximo de $U$, pero no tanto para que la convergencia ocurra en un tiempo razonable.
\begin{ejemplo}
Si 
\[c > M:=\max_{x \in E}\left(-U_x\right),\]
la convergencia está garantizada.
\end{ejemplo}

\textbf{Observaciones:} Típicamente, el algoritmo "recuerda" el $\hat{x}\in E$ visitado por $(X_n)_{n\geq 0}$ en que $U_{\hat{x}}$ es el máximo entre los visitados. Cuando se decide detener la cadena, digamos en el $n_{*}$, se retorna $\hat{x}$, no $X_{n_{*}}$.
\end{document}