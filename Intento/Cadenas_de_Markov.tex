
\section{Cadenas de Markov}
Una cadena de Markov es una sucesión de variables aleatorias $\left\{X_n;n=0,1,2,...\right\}$, definidas sobre algún espacio de probabilidad $\left(\Omega,\mathcal{
F}, \prob\right)$, tomando valores en un conjunto $E$ arbitrario, pero que supondremos que será finito o numerable, y que posee la propedad de Markov. Intuitívamente, una cadena de Markov posee la cualidad de que, conociendo el estado presente $X_n$, sin necesidad de conocer los estados anteriores puede hacer buenas predicciones acerca de los estados futuros.\\ Los siguientes temas presentan variadas aplicaciones de las cadenas de Markov. Tenga en cuenta que restringiremos el contenido a cadenas de Markov \textit{homogéneas}, aunque el caso no homogéneo sea necesario en muchas aplicaciones.

\subsection{Definiciones y propiedades elementales}

\begin{definicion}
Sea $E$ un conjunto finito o numerable. Un proceso estocástico $\{X_n;n\in \N\}$ es llamado \textit{Cadena de Markov} si, para todo $n\in\N$, la ley condicional de $X_{n+1}$ dado $X_0,X_1,\cdots,X_n$, es igual a la ley condicional de $X_{n+1}$ dado $X_n$.\\ \newline Esto es; $\forall\,x_0,\cdots,x_n\,\in E$:
\[\prob\left(X_{n+1}=x_{n+1}\,|\,X_o = x_0,\,X_1=x_1,\,\cdots,\,X_n=x_n\right) = \prob\left(X_{n+1}=x_{n+1}\,|\,X_n=x_n\right)\]
La cadena se dirá 'Homogénea', si lo anterior no depende del índice $n$. En cuyo caso, la matriz $P=\left(P_{xy}\right)_{x,y\in E}$, con $P_{x,y} = \prob\left(X_{n+1}=y\,|\,X_n = x\right)$\\ \newline
Además, llamamos $\mu\in \mathcal{P}(E)$ a $\mu_x = \prob(X_0 = x)$, denominada 'Distribución inicial'. Así, decimos que $\left(X_n\right)_{n\in\N}$ es una $(\mu,P)$-cadena de Markov homogénea.
\end{definicion}

\begin{ejemplo}
En $E=\Z$, si $Z_0,Z_1,Z_2,...$ son las ganancias de apuestas modeladas como variables aleatorias $i.i.d.$, entonces la ganancia total acumulada $X_n = \sum_{i=0}^nZ_i$ es una cadena de arkov ($C.M.$).
\end{ejemplo}

Un simple criterio, que nos permitirá en muchos casos verificar si un proceso dado es C.M., está dado por el siguiente lema.

\begin{lem}
Sean $E$ y $F$ dos conjuntos numerables, y sea $f$ un mapeo de $\N\times E \times F$ hacia $E$. Sean $X_0,Y_1,Y_2,...$, son variables aleatorias mútuamente independientes, siendo $X_0$ valor en $E$ e $Y_n$ valores en $F$.\\ 
Sean $\{X_n;\,n\geq 1\}$ un proceso en $E$, definido como:
\[X_{n+1} = f(n,X_n,Y_{n+1}),\hspace{0.7cm}n\in\N\]
Entonces $\{X_n;\,n\in\N\}$ es cadena de Markov.
\end{lem}

\textbf{Demostración: }
\[\prob\left(X_{n+1} = x_{n+1}\,|\,X_0=x_0,\,\cdots,\,X_n=x_n\right) = \frac{\prob\left(X_0=x_0,\,\cdots,\,X_n=x_n,\,X_{n+1}=x_{n+1}\right)}{\prob\left(X_0=x_0,\,\cdots,\,X_n=x_n\right)}\]
\[ = \sum_{\{z;f(n,x_n,z)=x_{n+1}\}}\frac{\prob\left(X_0=x_0,\,\cdots,\,X_n=x_n,\,Y_{n+1}=z\right)}{\prob\left(X_0=x_0,\,\cdots,\,X_n=x_n\right)}\]
Dado que $Y_{n+1}$ es una variable independiente a todos los valores $\{X_0,\cdots,X_n\}$, entonces:
\[ = \sum_{\{z;f(n,x_n,z)=x_{n+1}\}}\frac{\prob\left(X_0=x_0,\,\cdots,\,X_n=x_n,\,Y_{n+1}=z\right)}{\prob\left(X_0=x_0,\,\cdots,\,X_n=x_n\right)}\]
\[ = \sum_{\{z;f(n,x_n,z)=x_{n+1}\}}\frac{\prob\left(X_0=x_0,\,\cdots,\,X_n=x_n\right)\prob\left(Y_{n+1}=z\right)}{\prob\left(X_0=x_0,\,\cdots,\,X_n=x_n\right)}\]
\[ = \sum_{\{z;f(n,x_n,z)=x_{n+1}\}}\prob\left(Y_{n+1}=z\right)\]
\[=\frac{\prob\left(X_n = x_n,\,X_{n+1}=x_{n+1}\right)}{\prob\left(X_n =x_n\right)} = \prob\left(X_{n+1}=x_{n+1}\,|\,X_n=x_n\right)\]
\rule{0.7em}{0.7em}\\ \newline

Una cadena de Markov es el análogo de una sucesión determinista determinada por una relación de recurrencia del tipo
\[x_{n+1} = f(n,x_n),\]
opuesto al sistema 'con memoria', del tipo
\[x_{n+1} = f(n,x_n,x_{n-1},\cdots,x_0).\]
Acá la función $f(n,\cdot)$ es reemplazada por la matriz de trancisión
\[P_{xy}= \prob\left(X_{n+1}=y\,|\,X_n =x\right).\]
Hasta el momento, podemos asumir que la matriz $P=\left(P_{xy}:\,x,y\in E\right)$ es independiente de la variable tiempo $n$, una vez dicho que la cadena es \textit{homogénea}.

\begin{lem}
\label{Lema 8}
Sea $f:E\times [0,1]\rightarrow E$ medible, sea $X_0$ variable aleatoria en $E$, $Y_1,Y_2,\cdots,$ variables aleatorias uniformes en $[0,1]$, todas independientes entre sí (y de $X_0$). Entonces $\left(X_n\right)_{n\in\N}$ definido recursivamente por $X_{n+1} = f(X_n,Y_{n+1})$ es una $C.M.H.$ (cadena de Markov homogénea).
\end{lem}
 La matriz $P$ es llamada \textit{Markoviana} (o \textit{estocástica}), en el sentido en que tiene la siguiente propiedad; para cada $x\in E$, el vector fila $\left(P_{xy};\,y\in E\right)$ es una medida de probabilidad en $E$, en otras palabras;
 \[P_{xy}\geq 0,\hspace{0.2cm}\forall\,y\in E;\hspace{0.5cm}\sum_{y\in E}P_{xy} = 1\]
 El vector de distribución inicial, definido por $\mu_x = \prob\left(X_0 = x\right)$, se entiende como un vector fila, de modo que $\left(\mu P\right)_y = \sum_{x\in E}\mu_xP_{xy}$.\\ \newline
 
\begin{prop}
Para una $(\mu,P)$-$C.M.H.$ se cumplen:
\begin{enumerate}
    \item[i)] $\prob\left(X_0=x_0,\,\cdots,\,X_n=x_n\right) = \mu_{x_0}P_{x_0x_1}P_{x_1x_2}\cdots P_{x_{n-1}x_n}$.(Esta propiedad es equivalente a la definición de $(\mu,P)-C.M.H.$)
    \item[ii)] $\prob\left(X_{n+1}=x_{n+1},\,\cdots,\,X_{n+m}=x_{n+m}\,|\,X_0=x_0,\cdots,X_n=x_n\right) = P_{x_{n+1}x_{n+2}}\cdots P_{x_{n+m-1}x_{n+m}}$.
    \item[iii)] $\prob\left(X_n=y\,|\,X_0=x\right) = \left(P^n\right)_{xy}$ (Donde la notación anterior hace referencia a la potencia matricial).
    \item[iv)] $\prob\left(X_n = y\right) = \left(\mu P^n \right)_{y}$
    \item[v)] Para $g:E\rightarrow \R$, 
    \[\E\left(g(X_n)\,|\,X_0 = x\right) = \left(P^ng\right)_x = \sum_{y\in E} P^n_{xy} g(y)\]
    \item[vi)] $\E\left(g(X_n)\right) = \mu P^n g = \sum_{x,y\in E}\mu_x P^n_{xy}g_y$
\end{enumerate}
(En esta proposición hacemos uso de la notación $g_y = g(y)$).
\end{prop}

\textbf{Demostración: }\cite[págs. 20,21]{Pard}\\ \newline

\subsection{Propiedad de Markov fuerte}
Para la mejor comprensión de esta parte, recordemos la propiedad de Markov. Sea $\left\{X_n;n\in \N\right\}$ una cadena de Markov a valores en $E$ definida sobre el espacio de probabilidades $\left(\Omega,\mathcal{F},\prob\right)$. Dada una medida de probabilidad $\mu$ sobre $E$, usaremos la notación $\prob_{\mu}$ para denotar cualquier probabilidad sobre $\left(\Omega,\mathcal{F}\right)$ tal que, bajo $\prob_{\mu}$, la sucesión $\{X_n; n\geq 0\}$ es una cadena de Markov con ley inicial $\mu$; en otras palabras, $\mu$ es la ley de $X_0$, esto es,
\[\prob_{\mu}\left(X_0=x\right) = \mu_x, \hspace{0.5cm}x\in E.\]
parael caso $\mu = \delta_x$, escribiremos $\prob_x$ en vez de $\prob_{\delta_x}$.\\ $\prob_x$ puede ser interpretado como la ley condicional de $X$, dado $X_0=x$. Para cualquier $n\geq 0$, definimos $\mathcal{F}_n$ como la sigma-algebra de los eventos que están determinados por las variables $X_0,X_1,\cdots,X_n$, esto es,
\[\mathcal{F}_n = \left\{\{\omega; \left(X_0(\omega),\cdots,X_n(\omega)\right)\in B_n\};\,B_n \in 2^{E^{n+1}}\right\}\]
Donde $2^F$ denota la colección de todos los subconjuntos del conjunto $F$.

\begin{teorema}
Sea $\{X_n; n\geq 0\}$ una $(\mu,P)$ cadena de Markov. Entonces, para cada $n\in \N,$ $x\in E$, condicionalmente a $\{X_n=x\}$, $\{X_{n+p}; p\geq 0\}$ es una $(\delta_x,P)$-cadena de Markov independiente  de $\left(X_0,X_1,\cdots,X_n\right)$. En otras palabras, para todo $A \in \mathcal{F}_n$ y todo $m>0$, $x_1,\cdots,x_m \in E$,
\[\prob\left(A\cap\{X_{n+1}=x_1,\cdots,X_{n+m} = x_m\}|X_n=x\right)\]
\[= \prob\left(A|X_n=x\right)\prob_x\left(X_1 = x_1,\cdots,X_m = x_m\right)\]
\end{teorema}
\textbf{Demostración: }Es suficiente probar el resultado en el caso donde $A=\{X_0=y_0,X_1=y_1\cdots,X_n=y_n\}$ ($A$ es unión finita o numerable de conjuntos disjuntos de esta forma, y el resultado en el caso general saldrá de la $\sigma$-aditividad de $\prob$). Es suficiente considerar el caso $y_n =x$. Así, el lado izquiero de la igualdad es equivalente a
\[\frac{\prob\left(X_0=y_0,\cdots,X_n=x,X_{n+1}=x_1,\cdots,X_{n+m}=x_m\right)}{\prob\left(X_n=x\right)},\]
así, aplicando la propiedad anterior iterativamente, esto es igual a
\[\frac{\prob(A)}{\prob\left(X_n=x\right)}\times P_{xx_1}\times P_{x_1x_2}\times \cdots \times P_{x_{m-1}x_m},\]
o, en otras palabras,
\[\prob\left(A|X_n=x\right)\prob_x\left(X_1=x_1,\cdots,X_m=x_m\right).\]
\rule{0.7em}{0.7em}\\ \newline

El último resultado nos dice en particular que, el pasado y el futuro de una cadena son condicionalemnte independientes, dada la posición de la cadena en el tiempo presente $n$.\\ Ahora, nos gustaría expandir la propiedad de Markov, reemplazando el tiempo dado $n$ por un tiempo aleatorio (pero no cualquier tiempo aleatorio).

\begin{definicion}
Una variable aleatoria $T$ que toma valores en el conjunto $\N\cup \{+\infty\}$ es llamada tiempo de parada (t.d.p.) si, para todo $n\in \N$,
\[\{T=n\} \in \mathcal{F}_n.\]
\end{definicion}
En otras palabras, las observaciones $X_1,X_2,\cdots,X_n$, la trayectoria de la cadena en el tiempo $n$, es suficiente para decidir si $T$ es igual a $n$.

\begin{ejemplo}
    \begin{enumerate}
        \item Para cada $x\in E$, el tiempo de llegada al estado $x$,
        \[S_x = \begin{cases}
        \inf\{n\geq0;\,X_n=x\} & \text{, si existe $n$ que lo cumpla.}\\
        +\infty & \text{, otro caso.}
        \end{cases}\]
        y el tiempo del primer retorno al estado $x$,
        \[T_x = \begin{cases}
        \inf\ & \text{, si existe $n$ que lo cumpla.}\\
        +\infty & \text{, en otro caso.}
        \end{cases}\]
        son tiempo de parada\footnote{Con la convención de que el ínfimo de un conjunto vacío es $+\infty$, basta con escribir: $T_x = \inf\{n\geq 1;\,X_n=x\}$.}. En el caso de $T_x$ esto se cumple porque
        \[\{T_x = n\} = \{X_1\neq x\}\cap \cdots \cap \{X_{n-1}\neq x\}\cap\{X_n = x\}.\]
        \item $\forall\,A\subseteq E$, $T_A := \inf\{n\geq 0:\,X_n\in A\}$ es el tiempo de llegada al conjunto $A$. $T_A$ es t.d.p.
        \item $L_A:= \sup\{n\geq 1:\,X_n\in A\}$ es la última vez que  la cadena visita $A$. No es tiempo de parada, puesto que necesitamos conocer la trayectoria luego del tiempo $n$ para decidir si $L_A = n$, o no.
    \end{enumerate}
\end{ejemplo}\\ \newline \newline
Dado un tiempo de parada $T$, anotamos
\[\mathcal{F}_T := \{B \in \mathcal{F}\,|\,B\cap \{T=n\} \in \mathcal{F}_n,\, \foralln\}\]
Donde $\left(\Omega,\mathcal{F},\prob\right)$ es el espacio de probabilidades subyaccente.\\ (\textbf{Propuesto: }$\mathcal{F}_T$ es una $\sigma$-álgebra)

\begin{teorema}[Propiedad de Markov-fuerte.]
Sea \{X_n;\,n\geq 0\} una $(\mu,P)$-C.M., y $T$ un tiempo de parada. Condicionado a $\{T<\infty\}\cap\{X_T = x\}$, $\{X_{T+n};\,n\geq 0\}$ es una $(\delta_x,P)$-C.M. que es independiente de $\mathcal{F}_T$.\\ \newline
En otras palabras; para todo $A\in \mathcal{F}_T$ y todo $m>0$, $x_1,\cdots,x_m \in E$,
\[\prob\left(A\cap\{X_{T+1}=x_1,\cdots,X_{T+m}=x_m\}\,|\,X_T=x,\,T<\infty\right)\]
\[= \prob\left(A\,|\,X_T = x,\,T<\infty\right)\times\prob_x\left(X_1 = x_1,\cdots,X_m = x_m\right).\]
\end{teorema}\\ \newline
\textbf{Demostración: }Es suficiente mostrar que, para todo $n\in \N$,
\[\prob\left(A\cap\{T=n\}\cap\{X_{T+1}=x_1,\cdots,X_{T+m}=x_m\}\,|\,X_T=x\right)\]
\[= \prob\left(A\cap\{T=n\}\,|\,X_T = x\right)\prob_x\left(X_1 = x_1,\cdots,X_m=x_m\right),\]
que es resultado directo de la propiedad markoviana.\\ \newline
\rule{0.7em}{0.7em}\\ \newline

\subsection{Estados Recurrentes y Transientes }
Definimos $T_x = \inf\{n\geq 1;\,X_n = x\}$ como en  el ejemplo anterior.\\ \newline

\begin{definicion}
$x\in E$ se dice \textit{recurrente} si $\prob_x\left(T_x <\infty\right)=1$, y \textit{transiente} en el caso contrario (es decir, si $\prob_x\left(T_x<\infty\right)<1$).
\end{definicion}
 Definimos además, el número de retornos al estado $x$:
 \[N_x = \sum_{n\geq 1}\mathbbm{1}_{\{X_n = x\}}.\]
 
 \begin{prop}
    \begin{enumerate}
        \item[a)] Si $x$ es recurrente, entonces
        \[\prob_x\left(N_x = +\infty\right) = 1.\]
        \item[b)] Si $x$ es transiente, entonces
        \[\prob_x\left(N_x =k\right) = \left(1 - p_x\right)p_x^k,\hspace{0.3cm}k\geq 0,\]
        donde $p_x = \prob_x\left(T_x<\infty\right)$ (en particular, $N_x<\infty$, $\prob_x$-c.s.).
    \end{enumerate}
 \end{prop}\\ \newline
\textbf{Demostración: }Sea 
\[T_x^2 = \inf\{n> T_x;\,X_n=x\}\]
\[= T_x + \inf\{n\geq 1;\,X_{T+n}=x\}.\]
No es difícil ver que $T_x^2$ es un tiempo de parada:
\[\prob_x\left(T_x^2 < \infty\right) = \prob_x\left(T_x^2<\infty\,|\,T_x<\infty\right)\prob_x\left(T_x<\infty\right)\]
\[= \sum_{n=1}^{\infty}\prob_x\left(T_x^2= T_x + n\,|\,T_x<\infty\right)\prob_x\left(T_x <\infty\right).\]
Pero, por la propiedad de Markov fuerte, deducimos que
\[\porb_x\left(T_x^2 = T_x +n\,|\,T_x<\infty\right)\]
\[=\prob_x\left(X_{T_x +1}\neq x,\cdots,X_{T_x +n-1}\neq x,X_{T_x +n}=x\,|\,T_x<\infty\right)\]
\[= \prob_x\left(X_1\neq x,\cdots,X_{n-1}\neq x,X_n=x\right)\]
\[=\prob_x\left(T_x=n\right).\]
Finalmente,
\[\prob_x\left(T_x^2<\infty\right)=(\prob_x\left(T_x<\infty\right))^2\]
o
\[\prob_x\left(N_x\geq 2\right) = (\prob_x\left(T_x<\infty\right))^2.\]
Así, iterando el mismo argumento, deducimos que
\
Ambas afirmaciones de la proposición se deducen fácilmente de esta identidad.\\
\rule{0.7em}{0.7em}\\ \newline
\begin{cor}
$x$ es recurrente si y sólo si
\[\sum_{n=0}^{\infty}\left(P^n\right)_{xx} = +\infty.\]
\end{cor}
\textbf{Demostración: }Notemos que
\[\E_x(N_x) = \E_x\left(\sum_{n\geq 1}\mathbbm{1}_{\{X_n=x\}}\right) = \sum_{n\geq 1}\prob_x\left(X_n=x\right)\]
\[= \sum_{n\geq 1}\left(P^n\right)_{xx}.\]
Si $x$ es recurrente, entonces $N_x=\infty$ $\prob_x$-c.s., luego $\E(N_x)=\infty$, probando ($\Longrightarrow$).\\ \newline
Recíprocamente, si $x$ es transiente, por la parte \textbf{b)} de la proposición anterior, se tiene que $\E_x(N_x)<\infty$, con lo cual se prueba ($\Longleftarrow$).\\
\rule{0.7em}{0.7em}\\ \newline

\begin{definicion}
Decimos que un estado $y$ es accesible desde el estado $x$ (y anotamos $x\rightarrow y$), siempre que exista $n\geq 0$ tal que $\prob_x\left(X_n=y\right)>0$. Decimos que $x$ e $y$ se comunican (y escribimos $x\leftrightarrow y$) cuando $y\rightarrow x$,  además de $x\rightarrow y$. 
\end{definicion}

La relación $x\leftrightarrow y$ es una relación de equivalencia, entonces podemos particionar el conjunto $E$ en clases de equivalencia definidas por la relación $\leftrightarrow$.\\
Notamos que $x\leftrightarrow y$ $\Longleftrightarrow$ $\exists\,n\geq 0$ tal que $\left(P^n\right)_{xy}>0$, dado que $\prob_x\left(X_n=y\right)=\left(P^n\right)_{xy}$.  \\ \newline

\begin{teorema}
Sea $C\subset E$ una clase de equivalencia de la relación $\leftrightarrow$. Entonces todos los estados de $C$ son recurrentes, o bien, son todos transientes.
\end{teorema}

\textbf{Demostración: }\cite[págs. 26- Teo.4.5]{Pard}\\ \newline
Luego, las clases de equivalencia definidas por $\leftrightarrow$, se les llaman clases de recurrencia.

\begin{definicion}
Una $(\mu,P)$-C.M. se dice irreducible cuando $E$ consiste sólo de una clase de equivalencia . Se dirá irreducible y recurrente si es reducible y además esa calse de equivalencia es  recurrente.
\end{definicion}

\begin{prop}
Toda cadena de Markov irreducible sobre un espacio de estados $E$ finito, es recurrente.
\end{prop}

\textbf{Demostración: }Dado que $E$ es finito, al menos un estado debe ser visitado infinitas veces con probabilidad positiva, por lo tanto,  casi seguramente, por la proposición anterior, este estado (y todos los de la calse) es (son) recurrente.\\
\rule{0.7em}{0.7em}\\ \newline

\subsection{Casos Irreducibles y Recurrentes}
En esta sección, asumiremos que una cadena es irreducible y recurrente. Empezaremos estudiando las \textit{excursiones} de una cadena entre dos retornos sucesivos al mismo estado $x$:
\[\varepsilon_k = (X_{T_x^k},X_{T_x^k+1},\cdots,X_{T_x^{k+1}}),\hspace{0.3cm}k\geq 0.\]
Tales excursiones son secuencias aleatorias de largo aleatorio, al menos 2 y finito, compuesta de elementos $E\textbackslash{}\{x\}$, excepto por el primero y el último, que son iguales a $x$. Denotamos como $U$ al conjunto de secuencias
\[u = (x,x_1,\cdots,x_n,x),\]
con $n\geq 0$, $x_l\neq x$, $1\leq l \leq n$. $U$ es numerable, y es el conjunto de todas las posibles excursiones $\varepsilon_0,\varepsilon_1,...$. Por lo tanto, estas variables aleatorias toman valores en un conjunto numerable, y sus leyes de probabilidad están caracterizadas por las cantidades
\[\prob\left(\varepsilon_k = u\right),\hspace{0.3cm}u\in U.\]

\begin{prop}
Bajo $\prob_x$, la secuencia $(\varepsilon_0,\varepsilon_1,\cdots)$ de excursiones es $i.i.d.$, en otras palabras, existen probabilidades $\{p_u;\,u\in U\}$ sobre $U$ tal que, para todo $k>0$, $u_0,u_1,\cdots,u_k\,\in U$,
\[\prob_x\left(\varepsilon_0 = u_0,\,\varepsilon_1 = u_1,\,\cdots,\,\varepsilon_k = u_k\right) = \prod_{l=0}^k p_{u_l}.\]
\end{prop}

\textbf{Demostración: }Esto es consecuencia de la propiedad fuerte de Markov. En efecto,\newline $\{\varepsilon_0 = u_0\}\in \mathcal{F}_{T_x}$, y el evento
\[\{\varepsilon_1 = u_1,\cdots,\varepsilon_k = u_k\}\]
es de la forma
\[\{X_{T_x+1}=x_1,\cdots,X_{T_x+p}=x_p\},\]
pára algún $p>0$, $x_1,\cdots,x_p\,\in E$. En consecuencia,
\[\prob_x\left(\varepsilon_0 = u_0,\varepsilon_1 = u_1,\cdots, \varepsilon_k = u_k\right)\]
\[= \prob_x\left(\{\varepsilon_0 = u_0\}\cap\{X_{T_x+1}=x_1,\cdots,X_{T_x +p}=x_p\}\,|\,T_x <\infty\right)\]
\[\prob_x\left(\varepsilon_0 =u_0\right)\prob_x\left(X_1=u_1,\cdots,X_p =x_p\right)\]
\[=\prob_x\left(\varepsilon_0=u_0\right)\prob_x\left(\varepsilon_0=u_1,\cdots,\varepsilon_{k-1}=u_k\right)\]
\[=\prob_x\left(\varepsilon_0 =u_0\right)\prob_x\left(\varepsilon_0=u_1\right)\cdots\prob_x\left(\varepsilon_{k-1}=u_k\right)\]
\[=p_{u_0}p_{u_1}\cdots p_{u_k},\]
donde $\{p_u;\,u\in U\}$ es la ley de $\varepsilon_0$ bajo $\prob_x$.\\
\rule{0.7em}{0.7em}\\ \newline
Una medida (no necesariamente de probabilidad) sobre el conjunto $E$, es un 'vector fila' $\{\gamma_x;\,x\in E\}$ tal que $0\leq \gamma_x < \infty$, para todo $x$. Cuando la medida es finita, $\sum_{x\in E}\gamma_x <\infty$, podemos normalizarla, haciéndola medida de probabilidad sobre $E$, $\left(\gamma_x / \sum_{z\in E}\gamma_z\, ;\,x\in E\right)$.\\ Una medida $\gamma$ se dice \textit{invariente} (respecto a la matriz de transición $P$) cuando
\[\gamma P = \gamma,\]
esto es, 
\[\sum_{y\in E}\gamma_y P_{yx} = \gamma_x,\hspace{0.3cm}\forall\,x\in E.\]
Una medida $\gamma$ se dice \textit{estríctamente posita} si $\gamma_x >0$, para todo $x\in E$.\\ \newline

Una medida $\gamma$ es invariante si y sólo si  la cadena $(\gamma,P)$ cumple $\mathcal{L}(X_n)=\gamma$, $\forall\,n\in \N$. Esto es, $\gamma$ es la ley de $X_n$ para todo valor de $n$.\\ Dado que es para todo $n$, $\{X_{n+m};\,m\in\N\}$ también es $(\gamma,P)$-C.M.\\ \newline
\textbf{Observación: }Una \textbf{probabilidad} invariante es una medida de probabilidad  $\pi$, que satisface $\pi P = \pi$, o equivalentemente, para todo $x\in E$,
\[\sum_{y\neq x}\pi_y P_{yx} = \pi_x\left(1- P_{xx}\right)\]
esto es,
\[\prob\left(X_n\neq x ,X_{n+1}=x\right) = \prob\left(X_n = x,X_{n+1}\neq x\right)\]
lo que significa que en el equilibrio, el número medio de salidas  desde el estado $x$ entre los tiempo $n$ y $n+1$ es igual al número medio de llegadas al estado $x$ entre los mismos tiempos. Esta es la intuición que caracteriza a la probabilidad invariante.

\begin{teorema}
Sea $\{X_n\,;\,n\in N\}$ una C.M. irreducible y recurrente. Entonces existe una medida invariante $\gamma$ estrictamente positiva, cual es única salvo constantes multiplicativas.
\end{teorema}

\textbf{Demostración: }Para probar la existencia, sea $\gamma_y^x$ el número esperado de visitas al estado $y$ durante la excursión $\varepsilon_0$ partiendo de $x$, esto es,
\[\gamma_y^x = \E_x\left(\sum_{n=1}^{T_x}\mathbbm{1}_{X_n=y}\right)\]
\[= \sum_{n=1}^{\infty}\prob_x \left(X_n = y,\, n\leq T_x\right)\]
\[=\sum_{z\in E}\sum_{n=1}^{\infty}\prob_x(\{X_{n-1}=z,\,n-1<T_x\}\cap\{X_n = y\})\]
\[=\sum_{z\in E}\left(\sum_{n=2}^{\infty}\prob_x(X_{n-1} = z,\,n-1\leq T_x)\right)P_{zy}\]
\[= (\gamma^x P)_y\]
Notamos que es necesario usar la recurrencia para llegar a la penúltima igualdad. Ahora haremos uso de la irreducibilidad de la cadena. Existen $n,m$ tal que $(P^n)_{xy}>0$, $(P^m)_{yx}>0$. Por lo tanto, dado que $\gamma_x^x = 1$,
\[0<(P^n)_{xy} = \gamma_x^x (P^n)_{xy}\leq (\gamma^x P^n)_y = \gamma_y^x,\]
\[\gamma_y^x(P^n)_{yx}\leq (\gamma^x P^m)_x = \gamma_x^x = 1.\]
En consecuencia, $\gamma^x$ es una medida estrictamente positiva, que satisface $\gamma_x^x =1$.\\ Para asegurar la unicidad, sea $\lambda$ una medida invariante tal que $\lambda_x = 1$. Debemos mostrar primero que $\lambda \geq \gamma^x$, luego $\lambda = \gamma^x$. Notemos que esta parte de la demostración usa sólamente la irreducibilidad (y no la recurrencia). Tenemos
\[\lambda_y = P_{xy} + \sum_{z_1 \neq x}\lambda_{z_1}P_{z_1 y}\]
\[= P_{xy} + \sum_{z_1neq x}P_{xz_1}P_{z_1y} + \sum_{z_1,z_2 \neq x}\lambda_{z_2}P_{z_2z_1}P_{z_1y}\]
\[\geq \sum_{n=0}^{\infty}\sum_{z_1,\cdots,z_n\,\neq x}P_{xz_n}P_{z_nz_{n-1}}\cdots P_{z_1y}\]
\[= \sum_{n=0}^{\infty}\prob_x\left(X_{n+1} =y,\, T_x \geq n+1\right)\]
\[= \gamma_y^x.\]
Por lo tanto, definamos $\mu = \lambda -\gamma^x$ que también es una medida invariante, y $\mu_x =0$. Sea $y\in E$, y $n$ tal que $(P^n)_{yx}>0$. Entonces
\[0 = \mu_x = \sum_{z \in E}\mu_z(P^n)_{zx} \geq \mu_y(P^n)_{yx}.\]
Por lo tanto $mu_y = 0$, y esto se cumple para todo $y \in E$.\\
\rule{0.7em}{0.7em}\\ \newline
Sea $m_x = \E_x(T_x)$. Si esta cantidad es finita, entonces el estado $x$ se dirá \textit{recurrente positivo}, en otro caso se dirá \textit{recurrente nulo}.

\begin{teorema}
Sea $(X_n)_{n\in \N}$ una C.M. irreducible. Un estado $x$ es recurrente positivo si y sólo si todos los estados son recurrentes positivos, si y sólo si existe $\pi$ una probabilidad invariante.\\ \newline
Además, $\pi$ cumple que
\[\pi = (\pi_x = m_x^{-1},\,x\in E).\]
\end{teorema}

\textbf{Demostración: }Notemos que
\[m_x = \sum_{y\in E}\gamma_y^x.\]
Por lo tanto si $x$ es recurrente positivo, entonces la probabilidad $\pi = (\pi_y = \gamma_y^x/m_x,\,y\in E)$ es invariante.\\ 

Por otro lado, si $\pi$ es una probabilidad invariante, de la irreducibilidad, $\pi$ es estrictamente positiva\footnote{Ver demostración de teorema anterior.}, por lo tanto si $x$ es un estado arbitrario, $\lambda = (\lambda_y = \pi_y / \pi_x,\, y \in E)$ es una medida invariante que satisface $\lambda_x =1$. De la irreducibilidad y de la demostración de unicidad del teorema anterior,
\[m_x = \sum_{y\in E}\gamma_y^x = \sum_{y\in E}\frac{\pi_y}{\pi_x} = \frac{1}{\pi_x} <\infty.\]
Por lo tanto, $x$ es recurrente positiva, al igual que todos los demás estados.\\
\rule{0.7em}{0.7em}\\ \newline
Los dos último teoremas marcan una clara dicotomía en el siguiente sentido: en el caso \textit{irreducible y recurrente}, la cadena es \textit{recurrente positiva} cuando existe una \textit{probabilidad invariante}, y será \textit{recurrente nula} si una (por ende, todas) \textit{medida invariante} tiene masa total infinita ($\sum_i \pi_i = +\infty$). En particular, si $|E|<\infty$, entonces no existe estado recurrente nulo, más bien, cualquier estado recurrente es recurrente positivo.

\begin{cor}
Sea $\{X_n\}$ una cadena de Markov irreducible que es recurrente positiva. Para cualquier $x \in E$, asociamos $T_x =\inf\{n>0;\,X_n=x\}$. Entonces para todo $y\in E$,
\[\E_y(T_x) < \infty.\]
\end{cor}

\textbf{Demostración: }Notemos que
\[T_x \geq T_x\mathbbm{1}_{T_y<T_x},\]
de donde, tomando esperanza con respecto a $\prob_x$,
\[m_x \geq \E_x(T_x\,|\,T_y<T_x)\prob_x(T_y<T_x).\]
Pero, usando la propiedad fuerte de Markov; $\E_x(T_x\,|\,T_y<T_x) > \E_y(T_x)$, y de la irreducibilidad que $\prob_x(T_y<T_x) >0$, y concluímos.\\
\rule{0.7em}{0.7em}\\ \newline
\textbf{Observación: (caso no-irreducible)} Por simplicidad, consideremos sólo el caso $|E|<\infty$. Existe al menos una clase recurrente (que es recurrente positiva), por lo tanto existe al menos una probabilidad invariante. Cualquier probabilidad invariante cobra sólo estados recurrentes. Si sólo existe una clase recurrente, entonces la cadena posee sólo una probabilidad invariante. En otro caso, podemos asociarle a cada clase recurrente una única probabilidad invariante  con soporte en tal clase, y toda medida invariante son combinaciones convexas de estas, que conformarán puntos extremos de las combinaciones. Por lo tanto, si existen al menos dos clases recurrentes diferentes, existirá una cantidad no numerable de probabilidades invariantes.\\ \newline
Restrinjámonos, nuevamente, al caso irreducible. Ahora podemos enunciar el teorema ergódico para C.M.H., cual es una generalización de la L.G.N.

\begin{teorema}
Sea $(X_n)_{n\in\N}$ una C.M.H. irreducible y recurrente positiva. Sea $\pi = (\pi_x\,,\,x\in E)$ su única probabilidad invariante. Si $f:E\rightarrow \R$ es una función acotada, entonces
\[\frac{1}{n}\sum_{k=1}^n f(X_k)\, \xrightarrow{n\rightarrow \infty}\,\sum_{x\in E}f(x)\pi_x = \int_E f d\pi,\hspace{0.6cm}\prob-c.s.\]
\end{teorema}

\textbf{Demostración: }Como $f$ es acotada, existe $c$ tal que $|f(x)|\leq c$, para cada $x\in E$. Sea
\[N_x(n) = \sum_{1\leq k\leq n}\mathbbm{1}_{X_k=x}\]
el número de retornos al estado $x$ antes del tiempo $n$. Deseamos estudiar el límite, cuando $n\rightarrow \infty$ de
\[\frac{N_x(n)}{n}.\]
Sea $S_x^0,S_x^1,\cdots,S_x^k,\cdots$ los largos de las excursiones $\varepsilon_0,\varepsilon_1,\cdots, \varepsilon_k,\cdots$ partiendo del estado $x$. Tenemos
\[S_x^0 + \cdots + S_x^{N_x(n)-1}\leq n < S_x^0+\cdots + S_x^{N_x(n)}.\]
Por lo tanto
\[\frac{S_x^0 + \cdtos + S_x^{N_x(n)-1}}{N_x(n)}\leq \frac{n}{N_x(n)}\leq \frac{S_x^0 + \cdots + S_x^{N_x(n)}}{N_x(n)}.\]
Pero, dado que las variables aleatorias $\varepsilon_k$ son $i.i.d.$ (por lo tanto también lo son $S_x^k$), cuando $n\rightarrow \infty$,
\[\frac{S_x^0+\cdots+ S_x^{N_x(n)}}{N_x(n)} \rightarrow \E_x(T_x) = m_x,\hspace{0.3cm}\prob_x-c.s.\]
dado que $N_x(n)\rightarrow +\infty$, $\prob_x$-casi seguramente. Entonces, nuevamente por la ley de los grandes números,
\[\frac{n}{N_x(n)}\rightarrow m_x,\hspace{0.3cm}\prob_x-c.s.,\]
esto es,
\[\frac{N_x(n)}{n}\rightarrow\frac{1}{m_x},\hspace{0.3cm}\prob_x-c.s.\]
Esta convergencia también es cierta $\prob_{\mu}$-c.s., para toda ley inicial $\mu$, dado que el límite de $N_x(n)/n$ es el mismo para las cadenas $\{N_n;\,n\geq 0\}$ y $\{X_{T_x+n};\,n\geq 0\}$.\\ \newline
Ahora sea $F\subset E$. Definimos $\Bar{f} = \sum_{x\in E}\pi_xf(x)$, $c=\sup_x|f(x)|$. Tenemos que
\[\left| \frac{1}{n}\sum_{k=1}^n f(X_k) - \Bar{f}\right| = \left| \sum_{x\in E}\left(\frac{N_x(n)}{n}-\pi_x\right)f(x)\right|\]
\[\leq c\sum_{x\in F}\left|\frac{N_x(n)}{n}-\pi_x\right| + c\sum_{x\notin F}\left(\frac{N_x(n)}{n} + \pi_x\right)\]
\[= c\sum_{x\in F}\left|\frac{N_x(n)}{n} -\pi_x\right| + c\sum_{x\in F}\left(\pi_x - \frac{N_x(n)}{n}\right) + 2c\sum_{x \notin F}\pi_x\]
\[\leq 2c \sum_{x \in F}\left| \frac{N_x(n)}{n}-\pi_x\right| + 2c\sum_{x \notin F}\pi_x.\]
Basta con elegir un conjunto finito $F$ tal que $\sum_{x \notin F}\pi_x\leq \epsilon/4c$, y un $N(\omega)$ tal que, para todo $n\geq N(\omega)$,
\[\sum_{x\in F}\left|\frac{N_x(n)}{n}-\pi_x\right| \leq \frac{\epsilon}{4c},\]
así probamos el resultado.\\
\rule{0.7em}{0.7em}\\ \newline

\subsection{El caso aperiódico}
Ya hemos visto que en el caso irreducible y recurrente positivo;
\[\frac{1}{n}\sum_{k=1}^n\mathbbm{1}_{\{X_k = y\}}\rightarrow \pi_y\hspace{0.2cm}c.s.,\]
cuando $n\rightarrow \infty$. Tomando esperanza bajo $\prob_x$, deducimos que
\begin{equation}
\frac{1}{n}\sum_{k=1}^n\left(P^k\right)_{xy}\rightarrow \pi_y,\hspace{0.3cm}\forall\,x,y\in E.
\label{eq_cl12_1}
\end{equation}
La pregunta natural que surge es; si mantenemos las mismas hipótesis, ¿será que
\begin{equation}
    (P^n)_{xy}\xrightarrow{n \rightarrow \infty}\pi_y
    \label{eq_cl12_2}
\end{equation}
para todo $x,y\in E$?\\ \newline
En general, este resultado no es cierto.\\ 

Consideremos el paseo aleatorio sobre $E = \Z/N$, donde $N$ es un entero par (identificamos a $N$ con 0),
\[X_n = X_0 + Y_1 + \cdots + Y_n,\]
con $Y_n$ variables $i.i.d.$ tomando valores en $\{-1,1\}$, con igual robabilidad \\($\prob(Y_n = 1) = \prob(Y_n=-1) = 1/2$); en otras palabras,
\[X_n = (X_0 + Y_1 + \cdots + Y_n)\,\text{mod}\,N.\]
Esta cadena es irreducible, y recurrente positiva dado que $E$ es inito. Pero $(P^{2k+1})_{xx}=0$, para todo $x\in E$. En el caso particular de $N=2$, tenemos que $P^{2k} = I$ y $P^{2k+1} = P$.\\ Es decir, $(P^n)_{xx}$ no converge, aunque \ref{eq_cl12_1} si se cumple. Para obtener \ref{eq_cl12_2} necesitamos descartar este tipo de comportamientos.\\ \newline
Para que la convergencia deseada en \ref{eq_cl12_2} sea cierta, necesitamos una condición adicional.
\begin{definicion}
Un estado $x\in E$ se dice aperiódico si existe $N$ tal que 
\[(P^n)_{xx}>0,\hspace{0.3cm}\forall\,n\geq N.\]
\end{definicion}
\begin{lem}
Si $P$ es irreducible y existe un estado aperiódico $x$, entonces para todos $y,z \in E$, existe $M$ tal que $(P^n)_{yz} >0$, para todo $n\geq M$. En particular, todos los estados son aperiódicos.
\end{lem}

\textbf{Demostración: }De la irreducibilidad, existen $r,s\in \N$ tales que $(P^r)_{yx} >0$, $(P^s)_{xz}>0$. Más aún,
\[(P^{r+n+s})_{yz}\geq (P^r)_{yx}(P^n)_{xx}(P^s)_{xz}>0\]
si $n\geq N$. Entonces tenemos la propiedad deseada con $M=N+r+s$.\\
\rule{0.7em}{0.7em}\\ \newline
\textbf{Observación: }Supongamos el caso irreducible y recurrente positivo. Sea $\pi$ la probabilidad invariante, entonces $\pi_y>0$, para todo $y\in E$. Por lo tanto, el hecho de que exista $N$ tal que, para todo $n\geq N$, $(P^n)_{xy}>0$ es una condición necesaria para establecer la convergencia $(P^n)_{xy}\rightarrow \pi_y$. Ahora veremos que, además, es condición suficiente.\\ \newline
\begin{teorema}
Supongamos que $P$ es irreducible, recurrente positiva y aperiódica. Sea $\pi$ la única medida de probabilidad invariante. Si $\{X_n;\,n\in\N\}$ es una $(\mu,P)$-C.M., para todo $y\in E$,
\[\prob(X_n = y)\rightarrow \pi_y,\hspace{0.3cm}n\rightarrow \infty;\]
en otras palabras, 
\[(\mu P^n)_y \, \rightarrow\,\pi_y,\] 
para cualquier ley inicial $\mu$. En particular, para todos $x,y\in E$,
\[(P^n)_{xy}\rightarrow \pi_y.\]
\end{teorema}

\textbf{Demostración: }Debemos usar un par de argumentos. Sea $\{Y_n;\,n\in \N\}$ una $(\pi,P)$-C.M., independiente de  $\{X_n;\,n\in \N\}$, y $x\in E$ arbitrario. Sea
\[T = \inf\{n\geq 0;\,X_n=Y_n=x\}.\]
\begin{itemize}
    \item \textbf{Paso 1:} Mostrar que  $\prob(T<\infty)=1$.\\ \newline
    $\{W_n = (X_n,Y_n);\,n\in \N\}$ es una cadena da Markov a valores en $(E\times E)$, con ley inicial $\lambda$ (donde $\lambda_{(x,u)} = \mu_x\pi_u$) y matriz de transición $\Tilde{P}_{(x,u)(y,v)} = P_{xy}P_{uv}$. Dado que $P$ es periódica, para todo $x,u,y,v$, para todo $n$ suficientemente grande,
    \[(\Tilde{P}^n)_{(x,u)(y,v)} = (P^n)_{xy}(P^n)_{uv} > 0.\]
    Porque $\Tilde{P}$ es irreducible. Más aún, $\Tilde{P}$ posee probabilidad invariante
    \[\Tilde{\pi}_{(x,u)} = \pi_x\pi_u.\]
    Por lo tanto, $\Tilde{P}$    es recurrente positiva. $T$ es el momento del primer encuentro de la cadena $\{W_n\}$ con el punto $(x,x)$; y es finito casi seguramente.\\ \newline
    \item \textbf{Paso 2:} Definimos
    \[Z_n = \begin{cases}
            X_n &,\hspace{0.2cm}n\leq T;\\
            Y_n &,\hspace{0.2}n>T.
            \end{cases}\]
    Por la propiedad fuerte de Markov, ambos procesos $\{X_{T+n};\,n\geq 0\}$ y $\{T_{T+n};\,n\geq 0\}$ son $(\delta_x,P)$-C.M., independientes de $(X_0,\cdots,X_T)$. En consecuencia, $\{Z_n;\,n\in \N\}$ es, como $\{X_n\}$, una $(\mu,P)$-C.M.
    \newpage
    \item \textbf{Paso 3:} Concluyendo. con estas tres identidades
    \[\prob(Z_n=y)=\prob(X_n=y),\]
    \[\prob(Y_n=y)=\pi_y,\]
    \[\prob(Z_n=y)=\prob(X_n=y,\,n\leq T)+\prob(Y_n=y,\,n>T).\]
    Por lo tanto, 
    \[\left|\prob(X_n=y)-\pi_y\right| = \left|\prob(Z_n=y) - \prob(Y_n=y)\right| \leq \prob(n<T) \rightarrow 0,\]
    cuando $n\rightarrow \infty$.
\end{itemize}
\rule{0.7em}{0.7em}\\ \newline
\textbf{Observación: }Podemos definir el periodo del estado $x\in E$ como el máximo común divisor de los enteros $n$ tales que $(P^n)_{xx}>0$. Uno puede mostrar, con un argumento muy cercano al lema anterior, que cuando $P$ es reducible, todos los estados tienen el mismo periodo. Un estado se dice \textit{aperiódico} si su periodo es 1.\\ \newline
Ahora precisamos de la velocidad, o taza de convergencia del teorema anterior, bajo una supocisión adicional, llamada \textbf{condición de Doeblin}: existe $n_0\in \N$, $\beta>0$ y una probabilidad $\nu$ sobre $E$ tal que
\[(D)\hspace{1cm}(P^{n_0})_{xy}\geq \beta\nu_y,\hspace{0.2cm}\forall\,x,y\in E.\]
\textbf{Observación:} La condición $(D)$ es equivalente a la condición
\[\exists\,x\in E,\,n_0\geq 1\,\text{ tal que }\inf_{y\in E}(P^{n_0})_{yx} > 0.\]
Esto indica que el estado $x$ es aperiódico. Pero esto no implica la irreducibilidad. Queda como ejercicio mostrar que esto implica la existencia de una única clase recurrente y de una única probabilidad invariante.

\begin{lem}
Si $P$ es irreducible y aperiódica, y #E# es finito, entonces la condición $(D)$ se satisface.
\end{lem}

\textbf{Demostración: }Elegimos $x\in E$. Para cada $y\in E$, existe un $n_y$ tal que $n\geq n_y\,\Longrightarrow\, (P^n)_{yx}>0$. Sea
\[\Bar{n} = \sup_{y\in E}n_y,\hspace{0.3cm}\alpha = \inf_y (P^{\Bar{n}})_{yx}.\]
Entonces $\alpha >0$, y para todo $y\in E$,
\[(P^{\Bar{n}})_{yx}\geq \alpha.\]
Por lo tanto la condición $(D)$ se satisface con $n_0 = \Bar{n}$, $\beta=\alpha$, $\nu = \delta_x$.\\
\rule{0.7em}{0.7em}\\ \newline

Por otro lado, la condición de Doeblin es raramente satisfecha en el caso $|E| = +\infty$, dado que, típicamente, para todo $n\in \N$, $y\in E$,
\[\inf_{x\in E}(P^n)_{xy} = 0.\]

Para Obtener una taza de convergencia de $(P^n)_{xy}$ a $\pi_y$, necesitamos una forma de cuantificar esa convergencia.\\ Para ello utilizaremos la norma $l^1(\mathcal{P}(E))$ entre $\mu$ y $\nu$, medidas de probabilidad en $E$.\newline Por ejemplo,
\[||\mu-\nu ||_1 := \sum_{x\in E}|\mu_x - \nu_x|\]
Ahora introduciremos una herramienta que será de gran utilidad en las siguientes demostraciones.
\begin{definicion}
Un \textbf{coupling} de dos probabilidades $p$ y $q$ sobre $E$ es un par $(X,Y)$ de variables aleatorias a valores en $E$, tales que $p$ es la ley de $X$ y $q$ es la ley de $Y$.
\end{definicion}

\begin{lem}
Sean $p$ y $q$, dos probabilidades en $E$. 
\[||p-q||_1 = 2 \inf_{(X,Y)\,\text{coupling de }p,q}\prob(X\neq Y)\]
\end{lem}

\textbf{Demostración: }Primero, notemos que  si $(X,Y)$ es coupling de $p$ y $q$,
\[\prob(X=Y)=\sum_{x\in E}\prob(X=Y=x) \leq \sum_{x\in E}p_x\wedge q_x\]
Entonces,
\[\prob(X\neq Y)\geq 1- \sum_{x \in E}p_x\wedge q_x = \sum_{x\in E}(p_x-q_x)^{+}\]
y
\[||p-q||_1 = \sum_{x\in E}|p_x-q_x| \leq 2\prob(X\neq Y).\]
Por otro lado, definiendo $\alpha = \sum_{x \in E}p_x\wedge q_x$. Si $\xi, U,V$ y $W$ variables aleatorias mutuamente independientes tales que; $\xi \sim Bernoulli(\alpha)$, la ley de $U$ es $r$, definido por $r_x=\alpha^{-1}p_x\wedge q_x$, la ley de $V$ es $\Bar{p}$ definido por $\Bar{p}_x = (1-\alpha)^{-1}(p_x-q_x)^{+}$, y la ley de $W$, $\Bar{q}$, definida como $\Bar{q}_x = (1-\alpha)^{-1}(q_x-p_x)^{+}$, entonces 
\[X= \xi U + (1-\xi)V,\]
\[Y =\xi U +(1-\xi)W\]
es un coupling $(X,Y)$ de p y q.\\ En efecto, calculemos $\mathcal{L}(X)$:
\[\prob(X=x) = \prob(X=x\,|\,\xi=1)\prob(\xi = 1) + \prob(X=x\,|\,\xi=0)\prob(\xi=0)\]
\[= \prob(U=x)\prob(\xi = 1) + \prob(V=x)\prob(\xi=0)\]
\[= \frac{p_x\wedge q_x}{\alpha}\alpha + \frac{(p_x - q_x)^{+}}{1-\alpha}(1-\alpha)\]
\[= p_x\wedge q_x + (p_x -q_x)^{+} = p_x\]
Por lo tanto $\mathcal{L}(X) = p$. De la misma forma se muestra que $\mathcal{L}(Y) = q$.\\ \newline
Veamos que $\prob(X\neq Y) = (1-\alpha)$:
\[\prob(X\neq Y) = \prob(X\neq Y\,|\,\xi=1)\prob(\xi=1) + \prob(X\neq Y\,|\,\xi=0)\prob(xi=0)\]
\[= \cancel{\prob(U\neq U)}^{0}\prob(\xi=1) + \prob(V\neq W)\prob(\xi =0)\]
\[= \left[ 1-\prob(V=W)\right](1-\alpha)\]
\[= \left[1 - \sum_{x \in E}\prob(V=x,W=x)\right](1-\alpha)\]
\[= \left[1 - \sum_{x \in E}\prob(V=x)\prob(W=x)\right](1-\alpha)\]
\[=\left[ 1- \sum_{x\in E}\frac{(p_x -q_x)^{+}(q_x-p_x)^{+}}{(1-\alpha)^{2}}\right](1-\alpha)\]
\[=(1-\alpha)\]
De donde la última igualdad sale de que el producto $(p_x-q_x)^{+}(q_x-p_x)^{+} = 0$, $\forall\,x\in E$.\\ \newline
Luego, 
\[||p-q||_1 = \sum_{x\in E}|p_x-q_x| = \sum_{x\in E}[p_x+q_x-2(p_x\wedge q_x)]\]
\[= 2 - 2\sum_{x\in E}p_x\wedge q_x = 2-2\alpha = 2(1-\alpha) = 2\prob(X\neq Y).\]
\rule{0.7em}{0.7em}\\ \newline

\textbf{Observación: }Sea $(E,d)$ un espacio métrico cualquiera ($E$ no necesariamente inito o numerable), $\forall\,\mu,\nu$, probabilidades sobre $E$, poddemos definir
\[W_d(\mu,\nu) :=\inf_{\text{coupling de }\mu,\nu} \E\left[d(X,Y)\right]\]
Esto define una distancia sobre el espacio de medidas de probabilidad de $E$, llamada \textit{distancia de Wesserstein}. Notar que el caso en que la distancia del espacio sea $d(x,y)=\mathbbm{1}_{x\neq y}$, da lugar a $\E\left[d(x,y)\right] = \prob(X\neq Y)$.\\ \newline

El siguiente resultado entrega una taza de convergencia hacia la probabilidad invariante.

\begin{teorema}
Supongamos que $P$ es irreducible y satisface la condición de Doeblin $(D)$. Entonces $P$ es aperiódica, recurrente positiva, y si $\pi$ denota una probabilidad invariante, 
\[\sum_{y\in E}|(P^n)_{xy}-\pi_y| \leq 2(1-\beta)^{[n/n_0]},\hspace{0.3cm}\forall\,x\in E,\,\,n\in \N,\]
donde $[n/n_0]$ es la parte entera de $n/n_0$.
\end{teorema}

\textbf{Demostración: }Como la cadena es irreducible, la condición de Doeblin $(D)$ claramente implica que es aperiódica.
\begin{itemize}
    \item[Paso 1.] Primero probar que para cualesquiera dos probabilidades $\mu$ y $\nu$ en $E$,
    \begin{equation}
        ||\mu P^n - \nu P^n||_1 \leq 2(1-\beta)^{[n/n_0].}
        \label{clase14_dem1}
    \end{equation}
    Para probar esto, gracias al lema anterior, es suficiente contruir un coupling $(X_n,Y_n)$ de las probabilidades $\mu P^n$ y $\nu P^n$ tal que
    \[\prob(X_n \neq Y_n) \leq (1-\beta)^{[n/n_0]}.\]
    Supongamos que  $n = kn_0 + m$, con $m<n_0$. Dados $(X-_0,Y_0)$ con la ley $\mu \times \nu$ sobre $E\times E$, para $l = 0,1,2,\dcots,k-1$, definimos $(X_{(l+1)n_0},Y_{(l+1)n_0})$ en términos de $(X_{ln_0},Y_{ln_0})$ de la siguiente manera. Sea $\{\xi_l , U_l, V_l;\,l\geq 0\}$una sucesión de variables aleatorias mutuamente independientes, donde $\xi_l$ es una variable $Bernoulli$ con $\prob(\xi_l = 1)=\beta = 1 -\prob(\xi_l = 0)$, la ley de $U_l$ es $\Bar{m} = \beta^{-1}m$ y $V_l$ es uniforme en $[0,1]$. Definimos
    \[Q_{xy} = (-1\beta)^{-1}((P^{n_0})_{xy}-m_y)\]
    y $f:E\times [0,1] \rightarrow E$ tal que, para todo $x,y\in E$, $\{u;\,f(x,u)=y\}$ es un boreliano subconjunto de $[0,1]$, y dado que $V$ es uniforme en $[0,1]$, la ley de $f(x,V)$ es $Q_{x\cdot}$, $x\in E$. Ahora sean
    \[X_{(l+1)n_0} = \xi_l U_l + (1-\xi_l)f(X_{ln_0},V_l),\]
    \[Y_{(l+1)n_0} = \xi_l U_l + (1-\xi_l)f(Y_{ln_0},V_l).\]
    Notamos que, realmente, hemos contruído un coupling $(X_{ln_0},Y_{ln_0})$ de $\mu P^{ln_0}$ y $\nu P^{ln_0}$, para $l=0,cdtos,k$, tales que
    \[\prob(X_{ln_0} \neq Y_{ln_0}) \leq  \prob(\cap_{m=0}^l \xi_m =0) = (1-\beta)l.\]
    Resta por construir un coupling $(X_n,Y_n)$ de $\mu P^n$ y $\nu P^n$, tal que $\{X_n \neq Y_n\} \subset \{X_{kn_0} \neq Y_{kn_0}\}$, lo  que es fácil.
    \item[Paso 2.] Ahora probaremos que cualquier probabilidad $\mu$ sobre $E$, $\{\mu P^n;\,n\geq 0\}$ es una sucesión de Cauchy en el espacio de Banach, $l^1(E)$. Si $\nu = \mu P^m$, por \ref{clase14_dem1} 
    \[||\mu P^{n+m}-\mu P^n||_1 = ||\nu P^n - \mu P^n||_1 \leq 2c^{n-n_0},\]
    donde $c=(1-\beta)^{1/n_0}$. Se concluye.
    \item[Paso 3.] Del paso anterior tenemos que la secuencia de probabilidades $\{\mu P^n;\,n\geq 0\}$ converge en $l^1(E)$, hacia $\pi$, probabilidad en $E$. Pero
    \[\pi P = \lim_{n\rightarrow \infty}\mu P^{n+1} = \pi,\]
    por lo tanto $\pi$ es invariante, y la cadena es recurrente positiva. En consecuencia, de \ref{clase14_dem1}, para cada probabilidad $\mu$ de $E$,
    \[||\mu P^n - \pi||_1 \leq 2(1-\beta)^{[n/n_0]},\]
    lo que establece la razón de convergencia buscado, y la aperiodicidad.
\end{itemize}
\rule{0.7em}{0.7em}\\ \newline
Ahora enunciaremos un teorema central del límite para cadenas de Markov irreducibles, recurrentes positivas. Si tal cadena, además satisface
\[\sum_{y \in E}|(P^n)_{xy}-\pi_y| \leq Mt^n,\hspace{0.3cm}x \in E,\,n\in \N\]
con $M\in \R$ y $0<t<1$, es llamada \textit{uniformemente ergódica}. Sólamente debemos mostar que la condición de Doeblin implica uniforme ergodicidad. Esta propiedad implica el teorema centrar del límite.

\begin{teorema}
Sea $\{X_n;\,n\in \N\}$ es una cadena de Markov a valores de $E$, con matriz de de transición $P$ irreducible, que además es uniformemente ergódica y aperiódica. Sea $\pi$ la única distribución invariante de la cadena, y $f:E\rigtarrow \R$, tal que
\[\sum_{x\in E}\pi_x f^2(x) < \infty \hspace{0.2cm}\text{y}\hspace{0.2cm}\sum_{x \in E}\pi_x f(x) = 0.\]
Entonces, cuando $n\rigtarrow \infty$,
\[\frac{1}{\sqrt{n}}\sum_1^nf(X_k)\hspace{0.2cm}\xrightarrow{\,\,\mathcal{L}\,\,}\sigma_f Z,\]
donde $Z\sim \mathcal{N}(0,1)$ y
\[\sigma_f^2 = \sum_{x \in E}\pi_x (Qf)_x^2 - \sum_x \pi_x (PQf)_x^2\]
\[= 2\sum_x \pi_x (Qf)_x f_x - \sum_x \pi_x f_x^2,\]
con 
\[(Qf)_x = \sum_{n=0}^{\infty}\E_x\left[f(X_n)\right],\hspace{0.2cm}x\in E.\]
\end{teorema}

Note que la propiedad uniforme ergodicidad implica que la serie que define al operador $Q$ converja.

\subsection{Cadenas de Markov Reversibles}
Consideremos el caso irreducible y recurrente positivo. La propiedad de Markov (aquella que dice que el pasado y el futuro de la cadena son independientes condicionado al valor presente) nos dice que, cuando $\{X_n;\,n\in \N\}$ es una cadena de Markov, para cada cada $N$, $\{\hat{X}_n^N = X_{N-n};\,0\leq n \leq N\}$ también es cadena de Markov, a menos que $\{X_n\}$ inicie con probabilidad invariante $\pi$.
\begin{prop}
Sea $\{X_n;\,n\in\N\}$ una cadena de Markov con matriz de transición $P$, que supondremos irreducible, y $\pi$ su distribución invariante. Entonces la cadena a tiempo-reverso\\ $\{\hat{X}_n^N;\,0\leq n\leq N\}$ es una $(\pi,\hat{P})$-C.M., con
\[\pi_y \hat{P}_{yx} = \pi_x P_{xy},\hspace{0.3cm}x,y\in E.\]
\end{prop}

\textbf{Demostración: }
\[\prob(\hat{X}_{p+1} = x\,|\,\hat{X}_p = y)\]
\[= \prob(X_n =x\,|\,X_{n+1}=y)\]
\[= \prob(X_{n+1} = y\,|\,X_n=x)\times \frac{\prob(X_n=x)}{\prob(X_{n+1}=y)}\]
\rule{0.7em}{0.7em}\\ \newline

Decimos que la cadena $\{X_n;\,n\in\N\}$ es \textit{reversible} si $P=\hat{P}$, lo cual se satisface si y sólo si se satisfacen las \textit{ecuaciones de balance detallado}:
\[\pi_x P_{xy} = \pi_y P_{yx},\hspace{0.3cm}x,y\in E,\]
donde $\pi$ es la probabilidad invariante. Es fácil verificar que si $\pi$ satisface esa relación, entonces es $P$-invariante. La proposición contraria no necesariamente es cierta.\\ \newline
 
\textbf{Observación: }Si $\pi$ es la distribución invariante de una cadena de Markov irreducible (y, por lo tanto, recurrente positiva), la cadena no necesariamente es reversible. Suponga que $card(E)\geq 3$. Entonces podría existir $x\neq y$ tal que $P_{xy}=0\neq P_{yx}$. En consecuencia, $\pi_xP_{xy}=0\neq \pi_yP_{yx}$. La transición de $y$ a $x$ de la cadena original corresponde a la transición de $x$ a $y$ de la cadena a tiempo inverso, por lo tanto $P_{yx}\neq 0 \Longrightarrow \hat{P}_{xy} \neq 0$, de donde se concluye que $\hat{P}\neq P$.\\

\textbf{Observación: }Dada una matriz dde transición $P$ de una cadena de Markov irreducible, recurrente positiva, entonces una de las cosas que se busca es el cálculo de la probabilidad invariante. Este problema no es simpre resoluble.\\ 
Otro problema, es determinar una matriz de transición $P$, cuya cadena de Markov asociada admita una probabilidad $\pi$, invariante.\\ 
El segundo problema es bastante más fácil de resolver. En efecto, siempre existen varias soluciones. La manera más fácil de resolverlo es buscar $P$ tal que la cadena asociada es reversible con respecto a $\pi$. En otras palabras, es suficiente encontrar una matriz de transición $P$, irreducible, talq ue la cantidad $\pi_x P_{xy}$ sea simétrica en $x,y$.\\
A fin de resolver el primer problema, uno puede tratar de encontrar $\pi$ tal que
\[\pi_x P_{xy} = \pi_y P_{yx},\hspace{0.3cm}\forall\,x,y\in E,\]
lo que es diferente a resolver $\pi P= \pi$, no implica ninguna suma con respecto a $x$. Pero esta ecuación tiene solución sólo si la cadena es reversible con respecto a su única medida de probabilidad invariante, lo que podría no ser el caso.\\
Supongamos ahora que, dado el par $(P,\pi)$, deseamos verificar si $\pi$ es, o no, una probabilidad invariante con respecto a la matriz de transición $P$. Si la cantidad $\pi_x P_{xy}$ es simétrica en $x,y$, entonces la respuesta es que sí lo es, y ganamos una propiedad adicional, la reversibilidad. Si no es el caso, uno necesita verificar (o no) la igualdad $\pi P = \pi$. La siguiente proposición puede ser de utilidad para el problema de verificación.

\begin{prop}
Sea $P$ una matriz de transición irreducible, y $\pi$ una probabilidad en $E$, estrictamente positiva. Para cada par $x$, $y\in E$, definimos
\[\hat{P}_{xy}= \begin{cases}
                \frac{\pi_y}{\pi_x}P_{yx}, & \text{si }x\neq y,\\
                P_{xx}, & \text{si }x=y.
                \end{cases}\]
$\pi$ es probabilidad invariante de la cadena definida por la matriz de transición $P$, y $\hat{P}$ es la matriz de transición de la cadena a tiempo reverso si y sólo si, para cada $x\in E$,
\[\sum_{y\in E} \hat{P}_{xy} = 1.\]
\end{prop}

\subsection{Razón de convergencia al equilibrio}
Suponga el caso irreducible, recurrente positivo y aperiódico. Entonces, sabemos que para cada $x,y \in E$, $(Pn)_{x,y}\rightarrow pi_y$ cuando $n\rightarrow \infty$, donde $\pi$ es la única medida de probabilidad invariante. Más generalmente, esperamos que para una gran clase de funcione $f:E\rightarrow \R$, $(P^nf)_{x}\rightarrow \langle f,\pi \rangle$ cuando $n\rightarrow \infty$ para todo $x\in E$, donde, de ahora en adelante,
\[\langle f,\pi \rangle = \sum_{x\in E}f(x)\pi_x.\]
En esta sección discutiremos la razón a la cual esta convergencia se mantiene.

\subsubsection{El caso reversible de finitos estados}
En primer lugar, consideremos el caso simple. Asumamos que $E$ es finito (y escribimos $d=|E|$) y el proceso es reversible. Notemos que podemos identificar $L^2(\pi)$ con $\R^d$, dotado  del producto escalar
\[\langle f,g \rangle_{\pi} = \sum_{x\in E}f(x)g(x)\pi_x.\]
Ahora, la reversibilidad de $P$ es equivalente a decir que $P$ es, como elemento de $\mathcal{L}(L^2(\pi))$, un operador auto-adjunto, en el sentido que
\[\langle Pf,g\rangle_{\pi} = \sum_{x,y\in E}P_{x,y}f(y)g(x)\pi_x\]
\[= sum_{x,y\in E}P_{y,x}f(y)g(x)\pi_y\]
\[= \langle f,Pg \rangle_{\pi},\]
donde usamos la ecuación de balance detallado para la segunda identidad. Ahora veremos que la norma del operador $P$, como elemento de $\mathcal{L}(L^2(\pi))$, es almenos 1. En efecto, si $||\cdot ||_{\pi}$ denota la norma usual en $L^2(\pi)$,
\[||Pf||_{\pi}^2 = \sum_{x\in E}\left[(Pf)_x\right]^2\pi_x\]
\[ = \sum_{x\in E}(\E[f(X_t)\,|\,X_0=x])^2\pi_x\]
\[\leq \E[f^2(X_t),|\,X_0=x]\pi_x\]
\[=\sum_{x\im E}f^2(x)\pi_x,\]
donde se usó la desigualdad de Schwarz (o, equivalentemente, la desigualdad de Jensen) para la parte en que aparece la desigualdad, y la invarianza de $\pi$ para la última identidad.\\
A fin de poder trabajar en $\R^d$, dotado de la norma euclideana, introduciremos una nueva matriz $d\times d$
\[\Tilde{P}_{x,y} := \sqrt{\frac{\pi_x}{\pi_y}}P_{x,y}.\]
En notación matricial, $\Tilde{P}=\Pi^{1/2}P\Pi^{-1/2}$, donde $\Pi_{x,y} = \delta_{x,y}\pi_x$ es una matriz diagonal. Más aún, si $||\cdot ||$ es la norma euclideana en $\R^d$, para cada $f:\E\rightarrow \R$ (es decir, $f$ es una colección de números reales indexada por los $d$ elementos de $E$, en otras palabras un elemento de $\R^d$), anotando como $g = \Pi^{-1/2}f$, tenemos
\[||\Tilde{P}f||^2 = \sum_{x\in E}(P\Pi^{-1/2}f)_x^2 = ||Pg||_{\pi}^2 \leq ||g||_{\pi}^2 = ||f||^2.\]
Primero, notemos que $f$ es un vector propio de $\Tilde{P}$ si y sólo si $g=\Pi^{-1/2}f$ es un vector propio derecho de $P$, y $g' = \Pi^{1/2}f$ es un vector propio izquierdo de $P$ asociados al mismo valor propio. Tenemos que $\Tilde{P}$ es una $d\times d$ matriz simétrica, cuya norma está acotada por 1. Por lo tanto, por resultado elementales de álgebra lineal, $\Tilde{P}$ admite los valores propios $-1\leq \lambda_d \leq \lambda_{d-1}\leq \lambda_2\leq \lambda_1\leq 1$. Establecemos el siguiente lema.
\begin{lem}
Tenemos que $\lambda_2 <\lambda_1 = 1$ y $-1<\lambda_d$.
\end{lem}

\textbf{Demostración: }Ver \cite[Pag. 41]{Pard}\\ \newline

Sean $g_1, \cdots, g_d$ la base ortonormal de $L^2(\pi)$ hecha por vectores propios derechos de $P$, correspondientes respectivamente a los valores propos $1,\lambda_1,\cdots, \lambda_d$. Para cada $f\in L^2(\pi)$, dado $g_1 = (1,\cdots,1)$,
\[f-\langle f,\pi \rangle = \sum_{l=2}^d \langle f,g_l \rangle_{\pi}g_l,\]
\[Pf - \langle f,\pi \rangle = \sum_{l=2}^d \lambda_l \langle f,g_l \rangle_{\pi}g_l,\]
\[P^nf - \langle f,\pi \rangle = \sum_{l=2}^d \lambda_l^n \langle f,g_l \rangle_{\pi}g_l,\]
\[|| P^nf -\langle f,\pi \rangle||_{\pi}^2 = \sum_{l=2}^d \lambda_l^{2n}\langle f,g_l\rangle_{\pi}^2,\]
\[\leq \sup_{2\leq l\leq d}\lambda_{l}^{2n}||f-\langle f,\pi \rangle ||_{\pi}^2,\]
por lo tanto tenemos la siguiente proposición.
\begin{prop}
\[||P^nf - \langle f,\pi \rangle ||_{\pi} \leq (1-\beta)^n ||f-\langle f,\pi \rangle ||_{\pi},\]
donde $\beta := (1-\lambda_2)\wedge (1+\lambda_d)$ es la brecha espectral.
\end{prop}

\subsubsection{El caso general}
Más generalmente, lo mismo es cierto para
\[\beta := 1 - \sup_{f\in L^2(pi),\,||f||_{\pi} = 1} ||Pf - \lange f,pi \rangle||_{\pi}.\]
En eecto, con este $\beta$, considerando sólo el caso $f\neq 0$, dado que todas las desigualdades a conticnuación son ciertas para $f=0$, tenemos
\[||Pf -\langle f,\pi \rangle ||_{\pi} = \left|\left| P \left(\frac{f}{||f||_\pi}\right) - \left\langle \frac{f}{||f||_{\pi}},\pi\right\rangle \right|\right|_{\pi} \times ||f||_{\pi}\]
\[\leq (1-\beta)||f||_{\pi}.\]
Finalmente, la proposición anterior se mantiene en el caso general para esta nueva definición de $\beta$. Note que
\[||P^{n+1}f - \langle f,\pi \rangle||_{\pi} = ||P[P^nf - \langle f,\pi \rangle]||_{\pi}\]
\[\leq (1-\beta)||P^nf- \langle f,\pi \rangle||_{\pi}.\]
El resultado se concluye mediante inducción.

\subsection{Estadísticas en cadenas de Markov}
El objetivo de esta sección es introducir las nociones básicas de la estimación de los parámetros de una cadena de Markov.\\
Hemos visto que, para cada $n\geq 0 $, la ley del vector aleatorio $(X_0,X_1,\cdots,X_n)$ depende sólo de la ley inicial $\mu$ y de la matriz de transición $P$. Estamos interesados en las condiciones bajo las cuales uno puede estimar el par $(\mu,P)$, dado un vector de observaciones de $(X_0,X_1,\cdots,X_n)$, y de orma en que el error tienda a cero cuando $n\rightarrow \infty$.\\
Primero, analicemos el estimador de la probabilidad invariante $\mu$. Para cada $x\in E$, 
\[\hat{\mu}_x^n = \frac{1}{n+1}\sum_{l=0}^n\mathbbm{1}_{\{X_l =x\}}\]
es un estimador consistente de $\mu_x$, esto es consecuencia directa del teorema ergódico:
\begin{prop}
Para cada $x\in E$, $\hat{\mu}_x^n \rightarrow \mu_x$, casi seguramente cuando $n\rightarrow \infty$.
\end{prop}

Ahora observemos el estimador de $P_{xy}$ $,x,y\in E$. Elegimos el estimador
\[\hat{P}_{xy}^n = \frac{\sum_{l=0}^{n-1}\mathbbm{1}_{\{X_l= x,X_{l+1}=y\}}}{\sum_{l=0}^{n-1}\mathbbm{1}_{\{X_l =x\}}}.\]
Tenemos la siguiente proposición.
\begin{prop}
Para cada $x,y\in E$, $\hat{P}_{xy}^n \rightarrow P_{xy}$, casi seguramente cuando $n\rightarrow \infty$.
\end{prop}

\textbf{Demostración: }Ver \cite[Pag. 43]{Pard}\\ \newline
